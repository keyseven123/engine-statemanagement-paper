/*
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        https://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
*/

#include <Catalogs/Source/PhysicalSource.hpp>
#include <Catalogs/Source/PhysicalSourceTypes/CSVSourceType.hpp>
#include <Catalogs/Source/PhysicalSourceTypes/MemorySourceType.hpp>
#include <Common/DataTypes/DataTypeFactory.hpp>
#include <Configurations/Worker/QueryCompilerConfiguration.hpp>
#include <Runtime/Execution/ExecutablePipelineStage.hpp>
#include <Runtime/Execution/PipelineExecutionContext.hpp>
#include <Runtime/HardwareManager.hpp>
#include <Runtime/NesThread.hpp>
#include <Runtime/NodeEngine.hpp>
#include <Runtime/NodeEngineFactory.hpp>
#include <Runtime/WorkerContext.hpp>
#include <Sinks/SinkCreator.hpp>
#include <Sources/SourceCreator.hpp>
#include <random>
#include <tsl/robin_map.h>

#define RETURN_NOT_OK(s)                                                                                                         \
    do {                                                                                                                         \
        AsyncStatus _s = (s);                                                                                                    \
        if (_s != AsyncStatus::ok)                                                                                               \
            return _s;                                                                                                           \
    } while (0)

namespace Slash {
class abstract_zipfian_generator {
  public:
    virtual uint64_t operator()(std::mt19937& rng) = 0;
};

class ycsb_zipfian_generator : public abstract_zipfian_generator {
    static constexpr auto default_zipfian_factor = .99;

  public:
    explicit ycsb_zipfian_generator(uint64_t min_, uint64_t max_, double zipfian_factor = default_zipfian_factor)
        : ycsb_zipfian_generator(min_, max_, zipfian_factor, compute_zeta(0, max_ - min_ + 1, zipfian_factor, 0)) {
        // nop
    }

    explicit ycsb_zipfian_generator(uint64_t min_, uint64_t max_, double zipfianconstant_, double zetan_) : dist(0.0, 1.0) {
        num_items = max_ - min_ + 1;
        min = min_;
        theta = zipfianconstant = zipfianconstant_;

        theta = zipfianconstant;

        zeta2theta = zeta(0, 2, theta, 0);

        alpha = 1.0 / (1.0 - theta);
        zetan = zetan_;
        countforzeta = num_items;
        eta = (1 - std::pow(2.0 / num_items, 1 - theta)) / (1 - zeta2theta / zetan);
    }

    uint64_t operator()(std::mt19937& rng) override { return (*this)(rng, countforzeta); }

    uint64_t operator()(std::mt19937& rng, uint64_t new_item_count) {
        if (new_item_count > countforzeta) {
            // we have added more items. can compute zetan incrementally, which is cheaper
            num_items = new_item_count;
            zetan = zeta(countforzeta, num_items, theta, zetan);
            eta = (1 - std::pow(2.0 / num_items, 1 - theta)) / (1 - zeta2theta / zetan);
        }
        double u = dist(rng);
        double uz = u * zetan;
        if (uz < 1.0) {
            return min;
        }

        if (uz < 1.0 + std::pow(0.5, theta)) {
            return min + 1;
        }

        long ret = min + (long) ((num_items) *std::pow(eta * u - eta + 1, alpha));
        return ret;
    }

  private:
    double zeta(uint64_t st, uint64_t n, double thetaVal, double initialsum) {
        countforzeta = n;
        return compute_zeta(st, n, thetaVal, initialsum);
    }

    static double compute_zeta(uint64_t st, uint64_t n, double theta, double initialsum) {
        double sum = initialsum;
        for (auto i = st; i < n; i++) {
            sum += 1 / (std::pow(i + 1, theta));
        }
        return sum;
    }

  private:
    uint64_t num_items;
    uint64_t min;
    double zipfianconstant;
    double alpha, zetan, eta, theta, zeta2theta;
    uint64_t countforzeta;
    std::uniform_real_distribution<double> dist;
};
}// namespace Slash

namespace NES {

inline uint8_t ms_bit_scan_reverse(unsigned long* index, uint64_t mask) {
    bool found = mask > 0;
    *index = 63 - __builtin_clzl(mask);
    return found;
}

inline size_t next_power_of_two(size_t size) {
    // BSR returns the index k of the most-significant 1 bit. So 2^(k+1) > (size - 1) >= 2^k,
    // which means 2^(k+1) >= size > 2^k.
    unsigned long k;
    uint8_t found = ms_bit_scan_reverse(&k, size - 1);
    return (uint64_t) 1 << (found * (k + 1));
}

constexpr inline size_t pad_alignment(size_t size, size_t alignment) {
    //NES_VERIFY(alignment > 0, "wrong alignment");
    // Function implemented only for powers of 2.
    // NES_VERIFY((alignment & (alignment - 1)) == 0, "wrong alignment");
    size_t max_padding = alignment - 1;
    return (size + max_padding) & ~max_padding;
}

/// lazy join here

struct LeftStream {
    uint64_t key;
    uint64_t timestamp;
};

struct RightStream {
    uint64_t key;
    uint64_t timestamp;
};

// tagged address

class TaggedAddress {
  public:
    static constexpr uint64_t invalid_address = 1;

    /// A logical address is 8 bytes.
    /// --of which 48 bits are used for the address. (The remaining 16 bits are used by the hash
    /// table, for control bits and the tag.)
    static constexpr uint64_t address_bits = 48;
    static constexpr uint64_t max_address = (((uint64_t) 1) << address_bits) - 1;
    /// --of which 25 bits are used for offsets into a block, of size 2^25 = 32 MB.
    static constexpr uint64_t offset_bits = 25;
    static constexpr uint32_t max_offset = (((uint32_t) 1) << offset_bits) - 1;
    /// --and the remaining 23 bits are used for the block index, allowing for approximately 8 million
    /// blocks.
    static constexpr uint64_t block_bits = address_bits - offset_bits;
    static constexpr uint32_t max_block = (((uint32_t) 1) << block_bits) - 1;

  public:
    TaggedAddress(uint64_t code = 0) : _control(code) {}

    TaggedAddress(uint32_t block, uint32_t offset) : _offset(offset), _block(block) {}

    template<typename T>
    TaggedAddress(T* pointer) : _address(reinterpret_cast<uint64_t>(pointer)), _reserved(0) {}

    TaggedAddress(const TaggedAddress& other) : _control(other._control) {}

    TaggedAddress& operator=(const TaggedAddress& other) {
        _control = other._control;
        return *this;
    }

    inline uint64_t control() const { return _control; }

    inline uint64_t address() const { return _address; }

    inline uint8_t* pointer() const { return reinterpret_cast<uint8_t*>(_address); }

    inline uint32_t block() const { return _block; }

    inline uint32_t offset() const { return _offset; }

    inline bool operator!() { return _address == invalid_address; }

    TaggedAddress& operator+=(const TaggedAddress& that) {
        _control += that._control;
        return *this;
    }

    TaggedAddress& operator+=(uint64_t delta) {
        _control += delta;
        return *this;
    }

    friend bool operator==(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control == rhs._control; }

    friend bool operator!=(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control != rhs._control; }

    friend bool operator<(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control < rhs._control; }

    friend bool operator>(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control > rhs._control; }

    friend bool operator<=(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control <= rhs._control; }

    friend bool operator>=(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs._control >= rhs._control; }

    friend ptrdiff_t operator-(const TaggedAddress& lhs, const TaggedAddress& rhs) { return lhs.address() - rhs.address(); }

    friend TaggedAddress operator+(const TaggedAddress& lhs, const TaggedAddress& rhs) { return {lhs._control + rhs._control}; }

    template<typename O>
    friend O& operator<<(O& stream, const TaggedAddress& address) {
        stream << "TaggedAddress{offset: " << address._offset << ";block:" << address._block;
        stream << ";_address:" << address._address << "}";
        return stream;
    }

  private:
    union {
        struct {
            uint64_t _offset : offset_bits;         /// 25 bits
            uint64_t _block : block_bits;           /// 23 bits
            uint64_t __reserved : 64 - address_bits;/// 16 bits
        };
        struct {
            uint64_t _address : address_bits;      /// 48 bits
            uint64_t _reserved : 64 - address_bits;/// 16 bits
        };
        uint64_t _control;
    };
};

class AtomicTaggedAddress {
  public:
    explicit AtomicTaggedAddress() : _control(0) {}

    explicit AtomicTaggedAddress(TaggedAddress address) : _control(address.control()) {}

    AtomicTaggedAddress(const AtomicTaggedAddress& other) : _control(other.control()) {}

    inline TaggedAddress load() const { return TaggedAddress{_control.load()}; }

    inline void store(TaggedAddress value) { _control.store(value.control()); }

    inline TaggedAddress exchange(TaggedAddress value) { return _control.exchange(value.control()); }

    inline bool compare_exchange_strong(TaggedAddress& expected, TaggedAddress desired) {
        uint64_t expected_control = expected.control();
        bool result = _control.compare_exchange_strong(expected_control, desired.control());
        expected = TaggedAddress{expected_control};
        return result;
    }

    inline AtomicTaggedAddress& operator=(const AtomicTaggedAddress& other) {
        _control.store(other.control());
        return *this;
    }

    inline AtomicTaggedAddress& operator=(const TaggedAddress& other) {
        _control.store(other.control());
        return *this;
    }

    template<typename T>
    inline AtomicTaggedAddress& operator=(const T* ptr) {
        _control.store(reinterpret_cast<uintptr_t>(ptr));
        return *this;
    }

    inline TaggedAddress fetch_add(size_t offset) { return _control.fetch_add(offset); }

    inline uint64_t control() const { return load().control(); }

    inline uint64_t address() const { return load().address(); }

    inline uint32_t block() const { return load().block(); }

    inline uint32_t offset() const { return load().offset(); }

    inline uint8_t* pointer() const { return reinterpret_cast<uint8_t*>(load().address()); }

    std::atomic<uint64_t> _control;
};

// inner join state

class RecordHeader {
  protected:
    RecordHeader(uint64_t other) : _control(other) {}

    RecordHeader& operator=(uint64_t other) {
        _control = other;
        return *this;
    }

  public:
    RecordHeader(bool final_bit, bool tombstone, bool invalid, TaggedAddress previous_address)
        : _previous_address(previous_address.control()), _reserved{0}, _invalid{invalid}, _tombstone{tombstone}, _final_bit{
                                                                                                                     final_bit} {}

    RecordHeader(const RecordHeader& other) : _control(other._control) {}

    RecordHeader& operator=(const RecordHeader& other) {
        _control = other._control;
        return *this;
    }

    inline bool operator!() const { return _control == 0; }

    inline explicit operator bool() const { return _control != 0; }

    inline TaggedAddress previous_address() const { return TaggedAddress(_previous_address); }

    union {
        struct {
            uint64_t _previous_address : 48;
            uint64_t _reserved : 13;
            uint64_t _invalid : 1;
            uint64_t _tombstone : 1;
            uint64_t _final_bit : 1;
        };

        uint64_t _control;
    };
#ifdef ENABLE_DEBUG_IN_RELEASE
    //    uint64_t _magic_number;
#endif
};
#ifdef ENABLE_DEBUG_IN_RELEASE
static_assert(sizeof(RecordHeader) == 8, "sizeof(RecordHeader) != 16");
#else
static_assert(sizeof(RecordHeader) == 8, "sizeof(RecordHeader) != 8");
#endif

template<typename key_type, typename value_type>
class KVRecord {
    // To support records with alignment > 64, modify the persistent-memory allocator to allocate
    // a larger NULL page on startup.
    static_assert(alignof(key_type) <= 64, "alignof(key_type) > 64)");
    static_assert(alignof(value_type) <= 64, "alignof(value_type) > 64)");

  public:
    /// For placement new() operator. Can't set value, since it might be set by value = input (for
    /// upsert), or rmw_initial(...) (for RMW).
    KVRecord(RecordHeader header_, const key_type& key_) : header(header_) {
        void* buffer = const_cast<key_type*>(&key());
        new (buffer) key_type{key_};
    }

    /// Key appears immediately after record header (subject to alignment padding). Keys are
    /// immutable.
    inline constexpr const key_type& key() const {
        const auto* head = reinterpret_cast<const uint8_t*>(this);
        size_t offset = pad_alignment(sizeof(RecordHeader), alignof(key_type));
        return *reinterpret_cast<const key_type*>(head + offset);
    }

    /// Value appears immediately after key (subject to alignment padding). Values can be modified.
    inline constexpr const value_type& value() const {
        const auto* head = reinterpret_cast<const uint8_t*>(this);
        size_t offset =
            pad_alignment(sizeof(key_type) + pad_alignment(sizeof(RecordHeader), alignof(key_type)), alignof(value_type));
        return *reinterpret_cast<const value_type*>(head + offset);
    }
    inline constexpr value_type& value() {
        auto* head = reinterpret_cast<uint8_t*>(this);
        size_t offset =
            pad_alignment(sizeof(key_type) + pad_alignment(sizeof(RecordHeader), alignof(key_type)), alignof(value_type));
        return *reinterpret_cast<value_type*>(head + offset);
    }

    /// Size of the existing record, in memory. (Includes padding, if any, after the value.)
    inline constexpr uint32_t size() const { return size(sizeof(key_type), value().value_size()); }

    /// Size of a record to be created, in memory. (Includes padding, if any, after the value, so
    /// that the next record stored in the log is properly aligned.)
    static inline constexpr uint32_t size(const uint32_t key_size, uint32_t value_size) {
        return static_cast<uint32_t>(
            // --plus Value size, all padded to Header alignment.
            pad_alignment(value_size +
                              // --plus Key size, all padded to Value alignment.
                              pad_alignment(key_size +
                                                // Header, padded to Key alignment.
                                                pad_alignment(sizeof(RecordHeader), alignof(key_type)),
                                            alignof(value_type)),
                          alignof(RecordHeader)));
    }

  public:
    RecordHeader header;
};

static constexpr auto CHUNK_SIZE = 4096 - 64;

struct alignas(64) JoinStateChunk {
    std::atomic<uint32_t> pos{0};
    alignas(64) std::array<uint8_t, CHUNK_SIZE> data;
};

static_assert(sizeof(JoinStateChunk) == (4096));
static_assert(alignof(JoinStateChunk::data) == 64);
/// LSS Allocator

namespace lss_memory {

static constexpr uint32_t kSegmentSize = 8192;
static constexpr uint32_t kBaseAlignment = 16;

#ifdef RDMS_DEBUG
struct alignas(8) header {
    header(uint32_t size_, uint32_t offset_) : offset{offset_}, size{size_} {}

    /// Offset from the head of the segment allocator's buffer to the memory block.
    uint32_t offset;

    /// Size of the memory block.
    uint32_t size;
};
static_assert(sizeof(header) == 8, "header is not 8 bytes!");
#else
struct header {
    header(uint16_t offset_) : offset{offset_} {}

    /// Offset from the head of the segment allocator's buffer to the memory block.
    uint16_t offset;
};
static_assert(sizeof(header) == 2, "header is not 2 bytes!");
#endif

class thread_allocator;

class segment_state {
  public:
    segment_state() : control{0} {}

    segment_state(uint64_t control_) : control{control_} {}

    segment_state(uint32_t allocations_, uint32_t frees_) : frees{frees_}, allocations{allocations_} {}

    union {
        struct {
            /// Count of memory blocks freed inside this segment. Incremented on each free. Frees can
            /// take place on any thread.
            uint32_t frees;
            /// If this segment is sealed, then the count of memory blocks allocated inside this
            /// segment. Otherwise, zero.
            uint32_t allocations;
        };
        /// 64-bit control field, used so that threads can read the allocation count atomically at
        /// the same time they increment the free count atomically.
        std::atomic<uint64_t> control;
    };
};
static_assert(sizeof(segment_state) == 8, "sizeof(segment_state) != 8");
static_assert(kSegmentSize < UINT16_MAX / 2, "kSegmentSize too large for offset size!");

/// Allocation takes place inside segments. When a segment is no longer needed, we add it to the
/// garbage list.
class segment_allocator {
  public:
    /// Offset from the head of the class to the head of its buffer_ field.
#ifdef RDMS_DEBUG
    static constexpr uint32_t kBufferOffset = 8;
#else
    static constexpr uint32_t kBufferOffset = 14;
#endif

    /// Initialize the segment allocator and allocate the segment.
    segment_allocator() : state{} {
#ifdef RDMS_DEBUG
        // Debug LSS memory codes:
        //  - 0xBA - initialized, not allocated.
        std::memset(buffer, 0xBA, kSegmentSize);
#endif
    }

    /// Free the specified memory block. The block must be inside this segment! Returns true if the
    /// segment was freed; otherwise, returns false.
    void destroy(void* bytes);

    /// Seal the segment--no more blocks will be allocated inside this segment. Returns true if the
    /// segment was freed; otherwise, returns false.
    void seal(uint32_t blocks_allocated);

  private:
    /// Decrement the active references count, effectively freeing one allocation. Also frees the
    /// segment if (1) it is sealed and (2) its active references count is now zero. Returns true if
    /// the segment was freed; otherwise, returns false.
    void destroy();

  public:
    /// Segment allocator state (8 bytes).
    segment_state state;

    /// Padding, as needed, so that the first user allocation, at buffer_[sizeof(Header)] is 16-byte
    /// aligned.
    /// (In _DEBUG builds, sizeof(Header) == 8, so we require 0 bytes padding; in release builds,
    /// sizeof(Header) == 2, so we require 6 bytes padding.)
  private:
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wunused-private-field"
#ifdef _DEBUG
#else
    uint8_t padding_[6];
#endif
#pragma clang diagnostic pop
  public:
    /// This segment's memory. (First allocation's 8-byte Header starts at 8 (mod 16), so the
    /// allocation's contents will start at 0 (mod 16), as desired.)
    uint8_t buffer[kSegmentSize];
};

/// Allocator for a single thread. Allocates only; frees are directed by the global allocator
/// object directly to the relevant segment allocator.
class alignas(64) thread_allocator {
  public:
    static constexpr uint32_t kCacheLineSize = 64;

    /// Initialize the thread allocator. The real work happens lazily, when Allocate() is called for
    /// the first time.
    thread_allocator() : _segment_allocator{nullptr}, _segment_offset{0}, _allocations{0} {}

    /// Allocate a memory block of the specified size < kSegmentSize. If allocation fails, returns
    /// nullptr.
    void* allocate(uint32_t size);
    void* allocate_aligned(uint32_t size, uint32_t offset);

  private:
    inline uint32_t reserve(uint32_t block_size) {
        //RDMS_VERIFY2(block_size <= kSegmentSize, "Block size {} is less than {}", block_size, kSegmentSize);
        ++_allocations;
        uint32_t result = _segment_offset;
        //RDMS_VERIFY2(result <= kSegmentSize, "Block index {} is less than {}", result, kSegmentSize);
        _segment_offset += block_size;
        return result;
    }

    /// Segment inside which each thread's new allocations occur (pointer, 8 bytes).
    segment_allocator* _segment_allocator;

    /// Offset, into the active segment, of the next allocation.
    uint32_t _segment_offset;

    /// Number of blocks allocated inside the active segment.
    uint32_t _allocations;
};
static_assert(sizeof(thread_allocator) == 64, "sizeof(thread_allocator) != 64.");

}// namespace lss_memory

/// The LSS allocator allocates memory from a log-structured store, but does not perform garbage
/// collection. Memory is allocated from segments; each segment is freed only after all of its
/// allocations have been freed. This means that if a single allocation inside a segment is still
/// alive, the entire segment is still alive.
/// The LSS allocator works well in the case where memory usage is almost FIFO. In that case, all
/// of the segment's allocations will eventually be freed, so the segment will be freed. The LSS
/// allocator is intended to replace the (synchronous) function call stack, for asynchronous
/// continuations.
class lss_allocator {
  public:
    /// Maximum number of threads supported. For each possible thread, we reserve an 8-byte
    /// ThreadAllocator; so the memory required is 8 * (kMaxThreadCount) bytes. For each actual
    /// thread, we reserve a full SegmentAllocator, of size approximately kSegmentSize.
    static constexpr size_t kMaxThreadCount = Runtime::NesThread::MaxNumThreads;

    /// Size of each segment (in bytes).
    static constexpr uint32_t kSegmentSize = lss_memory::kSegmentSize;

    /// Preserving Windows malloc() behavior, all LSS allocations are aligned to 16 bytes.
    static constexpr uint32_t kBaseAlignment = lss_memory::kBaseAlignment;

    /// Initialize the LSS allocator. The real work happens lazily, when a thread calls Allocate()
    /// for the first time.
    lss_allocator() {
        for (size_t idx = 0; idx < kMaxThreadCount; ++idx) {
            thread_allocators_[idx] = lss_memory::thread_allocator();
        }
    }

    /// Allocate a memory block of the specified size. Note that size must be < kSegmentSize, since
    /// the allocation will take place inside a segment. The Allocate() code is ultimately single-
    /// threaded, since we maintain a separate ThreadAllocator per thread, each with its own
    /// SegmentAllocator. If allocation fails, returns nullptr.
    void* allocate(uint32_t size);
    void* allocate_aligned(uint32_t size, uint32_t alignment);

    /// Free the specified memory block. The Free() code is thread-safe, since the Free() request is
    /// always directed to the SegmentAllocator() that originally allocated the code--regardless of
    /// what thread it is issued from.
    void destroy(void* bytes);

  private:
    /// To reduce contention (and avoid needing atomic primitives in the allocation path), we
    /// maintain a unique allocator per thread.
    lss_memory::thread_allocator thread_allocators_[kMaxThreadCount];
};

lss_allocator _lss_allocator{};

lss_allocator& lss_alloc() { return _lss_allocator; }

namespace lss_memory {

static_assert(sizeof(header) < kBaseAlignment, "Unexpected header size!");

void segment_allocator::destroy(void*) { destroy(); }

void segment_allocator::seal(uint32_t allocations) {
    segment_state delta_state{allocations, 1};
    segment_state old_state{state.control.fetch_add(delta_state.control)};
    //RDMS_VERIFY2(old_state.allocations == 0, "wrong allocation count {}", old_state.allocations);
    //RDMS_VERIFY2(old_state.frees < allocations, "wrong free count {}<{}", old_state.frees, allocations);
    if (allocations == old_state.frees + 1) {
        // We were the last to free a block inside this segment, so we must free it.
        auto self = this;
        free(self);
    }
}

void segment_allocator::destroy() {
    segment_state delta_state{0, 1};
    segment_state old_state{state.control.fetch_add(delta_state.control)};
    //RDMS_VERIFY2(old_state.allocations == 0 || old_state.frees < old_state.allocations, "wrong allocation count {}<{}",
    //                 old_state.frees, old_state.allocations);
    if (old_state.allocations == old_state.frees + 1) {
        // We were the last to free a block inside this segment, so we must free it.
        auto self = this;
        free(self);
    }
}

void* thread_allocator::allocate(uint32_t size) {
    if (!_segment_allocator) {
        _segment_allocator = reinterpret_cast<segment_allocator*>(aligned_alloc(kCacheLineSize, sizeof(segment_allocator)));
        if (!_segment_allocator) {
            return nullptr;
        }
        new (_segment_allocator) segment_allocator{};
    }
    // Block is 16-byte aligned, after a 2-byte (8-byte in _DEBUG mode) header.
    uint32_t block_size = static_cast<uint32_t>(pad_alignment(size + sizeof(header), kBaseAlignment));
    uint32_t offset = reserve(block_size);
    if (_segment_offset <= kSegmentSize) {
        // The allocation succeeded inside the active segment.
        uint8_t* buffer = _segment_allocator->buffer;
#ifdef RDMS_DEBUG
        //  - 0xCA - allocated.
        ::memset(&buffer[offset], 0xCA, block_size);
#endif
        header* hdr = reinterpret_cast<header*>(&buffer[offset]);
#ifdef RDMS_DEBUG
        new (hdr) header(size, offset);
#else
        new (hdr) header(offset);
#endif
        return hdr + 1;
    } else {
        // We filled the active segment; seal it.
        _segment_allocator->seal(_allocations);
        _segment_allocator = nullptr;
        _allocations = 0;
        _segment_offset = 0;
        // Call self recursively, to allocate inside a new segment.
        return allocate(size);
    }
}

void* thread_allocator::allocate_aligned(uint32_t size, uint32_t alignment) {
    if (!_segment_allocator) {
        _segment_allocator = reinterpret_cast<segment_allocator*>(aligned_alloc(kCacheLineSize, sizeof(segment_allocator)));
        if (!_segment_allocator) {
            return nullptr;
        }
        new (_segment_allocator) segment_allocator{};
    }
    // Alignment must be >= base alignment, and a power of 2.
    assert(alignment >= kBaseAlignment);
    assert((alignment & (alignment - 1)) == 0);
    // Block needs to be large enough to hold the user block, the header, and the align land fill.
    // Max align land fill size is (alignment - kBaseAlignment).
    uint32_t block_size =
        static_cast<uint32_t>(pad_alignment(size + sizeof(header) + alignment - kBaseAlignment, kBaseAlignment));
    uint32_t block_offset = reserve(block_size);
    if (_segment_offset <= kSegmentSize) {
        // The allocation succeeded inside the active segment.
        uint8_t* buffer = _segment_allocator->buffer;
#ifdef RDMS_DEBUG
        //  - 0xEA - align land fill.
        ::memset(&buffer[block_offset], 0xEA, block_size);
#endif
        // Align the user block.
        uint32_t user_offset =
            static_cast<uint32_t>(pad_alignment(reinterpret_cast<size_t>(&buffer[block_offset]) + sizeof(header), alignment)
                                  - reinterpret_cast<size_t>(&buffer[block_offset]) - sizeof(header));
        assert(user_offset + sizeof(header) + size <= block_size);
        uint32_t offset = block_offset + user_offset;
#ifdef _DEBUG
        //  - 0xCA - allocated.
        ::memset(&buffer[offset], 0xCA, size + sizeof(Header));
#endif
        header* hdr = reinterpret_cast<header*>(&buffer[offset]);
#ifdef RDMS_DEBUG
        new (hdr) header(size, offset);
#else
        new (hdr) header(offset);
#endif
        return hdr + 1;
    } else {
        // We filled the active segment; seal it.
        _segment_allocator->seal(_allocations);
        _segment_allocator = nullptr;
        _allocations = 0;
        _segment_offset = 0;
        // Call self recursively, to allocate inside a new segment.
        return allocate_aligned(size, alignment);
    }
}

}// namespace lss_memory

void* lss_allocator::allocate(uint32_t size) { return thread_allocators_[Runtime::NesThread::getId()].allocate(size); }

void* lss_allocator::allocate_aligned(uint32_t size, uint32_t alignment) {
    return thread_allocators_[Runtime::NesThread::getId()].allocate_aligned(size, alignment);
}

void lss_allocator::destroy(void* bytes) {
    lss_memory::header* hdr = reinterpret_cast<lss_memory::header*>(bytes) - 1;
    auto* block = reinterpret_cast<uint8_t*>(hdr);
    uint32_t offset = hdr->offset + lss_memory::segment_allocator::kBufferOffset;
    auto* segment_allocator = reinterpret_cast<lss_memory::segment_allocator*>(block - offset);
    segment_allocator->destroy(bytes);
}

/// constants

enum class AsyncStatus : uint8_t {
    ok = 0,
    pending = 1,
    not_found = 2,
    out_of_memory = 3,
    io_error = 4,
    corruption = 5,
    aborted = 6,
};

enum class BackendAction : uint8_t {
    None = 0,
    ResetIndex,
    Destroy,
};

enum class StateBackendPhase : uint8_t { Rest = 0, ResetPrepare, ResetInProgress, Closed, Invalid };

///// allocation

template<typename T = void>
T* allocAligned(size_t size, size_t alignment = 16) {
    void* tmp = nullptr;
    NES_ASSERT(0 == posix_memalign(&tmp, alignment, size), "Cannot allocate");
    return reinterpret_cast<T*>(tmp);
}

/// async context

template<typename T>
using remove_const_t = typename std::remove_const<T>::type;

/// alloc_aligned(): allocate a unique_ptr with a particular alignment.
template<typename T>
void unique_ptr_aligned_deleter(T* p) {
    auto q = const_cast<remove_const_t<T>*>(p);
    free(q);
}

namespace detail {
template<typename T>
struct aligned_deleter {
    void operator()(T* p) const { unique_ptr_aligned_deleter(p); }
};
}// namespace detail
template<typename T>
using aligned_unique_ptr_t = std::unique_ptr<T, detail::aligned_deleter<T>>;
static_assert(sizeof(aligned_unique_ptr_t<void>) == 8, "sizeof(unique_aligned_ptr_t)");

template<typename T>
aligned_unique_ptr_t<T> make_aligned_unique_ptr(T* p) {
    return aligned_unique_ptr_t<T>(p, detail::aligned_deleter<T>());
}

template<typename T>
aligned_unique_ptr_t<T> alloc_aligned(size_t alignment, size_t size) {
    return make_aligned_unique_ptr<T>(allocAligned<T>(size, alignment));
}

/// alloc_context(): allocate a small chunk of memory for a callback context.
template<typename T>
void unique_ptr_context_deleter(T* p) {
    auto q = const_cast<remove_const_t<T>*>(p);
    q->~T();
    lss_alloc().destroy(q);
}

namespace detail {
template<typename T>
struct context_deleter {
    void operator()(T* p) const { unique_ptr_context_deleter(p); }
};
}// namespace detail
template<typename T>
using context_unique_ptr_t = std::unique_ptr<T, detail::context_deleter<T>>;
static_assert(sizeof(context_unique_ptr_t<void>) == 8, "sizeof(context_unique_ptr_t)");

template<typename T>
context_unique_ptr_t<T> make_context_unique_ptr(T* p) {
    return context_unique_ptr_t<T>(p, detail::context_deleter<T>());
}

template<typename T>
context_unique_ptr_t<T> alloc_context(uint32_t size) {
    return make_context_unique_ptr<T>(reinterpret_cast<T*>(lss_alloc().allocate(size)));
}

/// Standard interface for contexts used by async callbacks.
class AsyncContextBase {
  public:
    static constexpr bool has_variable_size = false;

  public:
    AsyncContextBase() : _from_deep_copy(false) {
        // nop
    }

    virtual ~AsyncContextBase() {}

    /// Contexts are initially allocated (as local variables) on the stack. When an operation goes
    /// async, it deep copies its context to a new heap allocation; this context must also deep copy
    /// its parent context, if any. Once a context has been deep copied, subsequent DeepCopy() calls
    /// just return the original, heap-allocated copy.
    AsyncStatus deep_copy(AsyncContextBase*& context_copy) {
        if (_from_deep_copy) {
            // Already on the heap: nothing to do.
            context_copy = this;
            return AsyncStatus::ok;
        } else {
            RETURN_NOT_OK(deep_copy_internal(context_copy));
            context_copy->_from_deep_copy = true;
            return AsyncStatus::ok;
        }
    }

    /// Whether the internal state for the async context has been copied to a heap-allocated memory
    /// block.
    bool from_deep_copy() const { return _from_deep_copy; }

  protected:
    /// Override this method to make a deep, persistent copy of your context. A context should:
    ///   1. Allocate memory for its copy. If the allocation fails, return Status::OutOfMemory.
    ///   2. If it has a parent/caller context, call DeepCopy() on that context. If the call fails,
    ///      free the memory it just allocated and return the call's error code.
    ///   3. Initialize its copy and return Status::Ok..
    virtual AsyncStatus deep_copy_internal(AsyncContextBase*& context_copy) = 0;

    /// A common pattern: deep copy, when context has no parent/caller context.
    template<class C>
    inline static AsyncStatus deep_copy_internal(C& context, AsyncContextBase*& context_copy) {
        context_copy = nullptr;
        auto ctxt = alloc_context<C>(sizeof(C));
        if (!ctxt.get()) {
            return AsyncStatus::out_of_memory;
        }
        new (ctxt.get()) C{context};
        context_copy = ctxt.release();
        return AsyncStatus::ok;
    }
    /// Another common pattern: deep copy, when context has a parent/caller context.
    template<class C>
    inline static AsyncStatus deep_copy_internal(C& context, AsyncContextBase* caller_context, AsyncContextBase*& context_copy) {
        context_copy = nullptr;
        auto ctxt = alloc_context<C>(sizeof(C));
        if (!ctxt.get()) {
            return AsyncStatus::out_of_memory;
        }
        AsyncContextBase* caller_context_copy;
        RETURN_NOT_OK(caller_context->deep_copy(caller_context_copy));
        new (ctxt.get()) C{context, caller_context_copy};
        context_copy = ctxt.release();
        return AsyncStatus::ok;
    }

  private:
    /// Whether the internal state for the async context has been copied to a heap-allocated memory
    /// block.
    bool _from_deep_copy;
};

/// User-defined callbacks for async FASTER operations. Async callback equivalent of:
///   Status some_function(context* arg).
typedef void (*AsyncCallbackFn)(AsyncContextBase* ctxt, AsyncStatus result);

/// Helper class, for use inside a continuation callback, that ensures the context will be freed
/// when the callback exits.
template<class C>
class CallbackContext {
  public:
    CallbackContext(AsyncContextBase* context) : async{false} { ctx = make_context_unique_ptr(static_cast<C*>(context)); }

    ~CallbackContext() {
        if (async || !ctx->from_deep_copy()) {
            // The callback went async again, or it never went async. The next callback or the caller is
            // responsible for freeing the context.
            ctx.release();
        }
    }

    C* get() const { return ctx.get(); }
    C* operator->() const { return ctx.get(); }

  public:
    bool async;

  protected:
    context_unique_ptr_t<C> ctx;
};

//// light epoch manager

class LightEpochManager {
  private:
    /// Entry in epoch table
    struct alignas(64) EpochEntry {
        EpochEntry() : local_current_epoch{0}, reentrant{0}, phase_finished{StateBackendPhase::Rest} {}

        uint64_t local_current_epoch;
        uint32_t reentrant;
        std::atomic<StateBackendPhase> phase_finished;
    };
    static_assert(sizeof(EpochEntry) == 64, "sizeof(EpochEntry) != 64");

    struct EpochAction {
        typedef void (*callback_t)(AsyncContextBase*);

        static constexpr uint64_t kFree = UINT64_MAX;
        static constexpr uint64_t kLocked = UINT64_MAX - 1;

        EpochAction() : epoch{kFree}, callback{nullptr}, context{nullptr} {}

        void reset() {
            callback = nullptr;
            context = nullptr;
            epoch = kFree;
        }

        bool is_free() const { return epoch.load() == kFree; }

        bool try_pop(uint64_t expected_epoch) {
            bool retval = epoch.compare_exchange_strong(expected_epoch, kLocked);
            if (retval) {
                callback_t _callback = callback;
                AsyncContextBase* context_ = context;
                callback = nullptr;
                context = nullptr;
                // Release the lock.
                epoch.store(kFree);
                // Perform the action.
                _callback(context_);
            }
            return retval;
        }

        bool try_push(uint64_t prior_epoch, callback_t new_callback, AsyncContextBase* new_context) {
            uint64_t expected_epoch = kFree;
            bool retval = epoch.compare_exchange_strong(expected_epoch, kLocked);
            if (retval) {
                callback = new_callback;
                context = new_context;
                // Release the lock.
                epoch.store(prior_epoch);
            }
            return retval;
        }

        bool try_swap(uint64_t expected_epoch, uint64_t prior_epoch, callback_t new_callback, AsyncContextBase* new_context) {
            bool retval = epoch.compare_exchange_strong(expected_epoch, kLocked);
            if (retval) {
                callback_t existing_callback = callback;
                AsyncContextBase* existing_context = context;
                callback = new_callback;
                context = new_context;
                // Release the lock.
                epoch.store(prior_epoch);
                // Perform the action.
                existing_callback(existing_context);
            }
            return retval;
        }

        /// The epoch field is atomic--always read it first and write it last.
        std::atomic<uint64_t> epoch;

        void (*callback)(AsyncContextBase* context);
        AsyncContextBase* context;
    };

  public:
    /// Default invalid page_index entry.
    static constexpr uint32_t kInvalidIndex = 0;
    /// This thread is not protecting any epoch.
    static constexpr uint64_t kUnprotected = 0;

  private:
    /// Default number of entries in the entries table
    static constexpr uint32_t kTableSize = Runtime::NesThread::MaxNumThreads;
    /// Default drainlist size
    static constexpr uint32_t kDrainListSize = 256;
    /// Epoch table
    EpochEntry* _table;
    /// Number of entries in epoch table.
    uint32_t _num_entries;

    /// List of action, epoch pairs containing actions to performed when an epoch becomes
    /// safe to reclaim.
    EpochAction _drain_list[kDrainListSize];
    /// Count of drain actions
    std::atomic<uint32_t> _drain_count;

  public:
    /// Current system epoch (global state)
    std::atomic<uint64_t> current_epoch;
    /// Cached value of epoch that is safe to reclaim
    std::atomic<uint64_t> safe_to_reclaim_epoch;

    LightEpochManager(uint32_t size = kTableSize) : _table{nullptr}, _num_entries{0}, _drain_list{}, _drain_count{0} {
        reset(size);
    }

    ~LightEpochManager() { destroy(); }

  private:
    void reset(uint32_t size) {
        _num_entries = size;
        // do cache-line alignment
        _table = allocAligned<EpochEntry>(size + 2, 64);
        current_epoch = 1;
        safe_to_reclaim_epoch = 0;
        for (uint32_t idx = 0; idx < kDrainListSize; ++idx) {
            _drain_list[idx].reset();
        }
        _drain_count = 0;
    }

    void destroy() {
        free(_table);
        _num_entries = 0;
        current_epoch = 1;
        safe_to_reclaim_epoch = 0;
    }

  public:
    /// Enter the thread into the protected code region
    inline uint64_t protect() {
        uint32_t entry = Runtime::NesThread::getId();
        _table[entry].local_current_epoch = current_epoch.load();
        return _table[entry].local_current_epoch;
    }

    /// Enter the thread into the protected code region
    /// Process entries in drain list if possible
    inline uint64_t protect_and_drain() {
        uint32_t entry = Runtime::NesThread::getId();
        _table[entry].local_current_epoch = current_epoch.load();
        if (_drain_count.load() > 0) {
            drain(_table[entry].local_current_epoch);
        }
        return _table[entry].local_current_epoch;
    }

    uint64_t reentrant_protect() {
        uint32_t entry = Runtime::NesThread::getId();
        if (_table[entry].local_current_epoch != kUnprotected)
            return _table[entry].local_current_epoch;
        _table[entry].local_current_epoch = current_epoch.load();
        _table[entry].reentrant++;
        return _table[entry].local_current_epoch;
    }

    inline bool is_protected() {
        uint32_t entry = Runtime::NesThread::getId();
        return _table[entry].local_current_epoch != kUnprotected;
    }

    /// Exit the thread from the protected code region.
    void unprotect() { _table[Runtime::NesThread::getId()].local_current_epoch = kUnprotected; }

    void reentrant_unprotect() {
        uint32_t entry = Runtime::NesThread::getId();
        if (--(_table[entry].reentrant) == 0) {
            _table[entry].local_current_epoch = kUnprotected;
        }
    }

    void drain(uint64_t next_epoch) {
        compute_reclaimable_epoch(next_epoch);
        for (uint32_t idx = 0; idx < kDrainListSize; ++idx) {
            uint64_t trigger_epoch = _drain_list[idx].epoch.load();
            if (trigger_epoch <= safe_to_reclaim_epoch) {
                if (_drain_list[idx].try_pop(trigger_epoch)) {
                    if (--_drain_count == 0) {
                        break;
                    }
                }
            }
        }
    }

    /// Increment the current epoch (global system state)
    uint64_t bump_epoch() {
        uint64_t next = ++current_epoch;
        if (_drain_count > 0) {
            drain(next);
        }
        return next;
    }

    /// Increment the current epoch (global system state) and register
    /// a trigger action for when older epoch becomes safe to reclaim
    uint64_t bump_epoch(EpochAction::callback_t callback, AsyncContextBase* context) {
        uint64_t prior_epoch = bump_epoch() - 1;
        uint32_t i = 0, j = 0;
        while (true) {
            uint64_t trigger_epoch = _drain_list[i].epoch.load();
            if (trigger_epoch == EpochAction::kFree) {
                if (_drain_list[i].try_push(prior_epoch, callback, context)) {
                    ++_drain_count;
                    break;
                }
            } else if (trigger_epoch <= safe_to_reclaim_epoch.load()) {
                if (_drain_list[i].try_swap(trigger_epoch, prior_epoch, callback, context)) {
                    break;
                }
            }
            if (++i == kDrainListSize) {
                i = 0;
                if (++j == 500) {
                    j = 0;
                    std::this_thread::sleep_for(std::chrono::seconds(1));
                    NES_INFO("Slowdown: Unable to add trigger to epoch");
                }
            }
        }
        return prior_epoch + 1;
    }

    /// Compute latest epoch that is safe to reclaim, by scanning the epoch table
    uint64_t compute_reclaimable_epoch(uint64_t current_epoch_) {
        uint64_t oldest_ongoing_call = current_epoch_;
        for (uint32_t index = 1; index <= _num_entries; ++index) {
            uint64_t entry_epoch = _table[index].local_current_epoch;
            if (entry_epoch != kUnprotected && entry_epoch < oldest_ongoing_call) {
                oldest_ongoing_call = entry_epoch;
            }
        }
        safe_to_reclaim_epoch = oldest_ongoing_call - 1;
        return safe_to_reclaim_epoch;
    }

    void spin_wait_reclaimable(uint64_t current_epoch_, uint64_t safe_to_reclaim_epoch_) {
        do {
            compute_reclaimable_epoch(current_epoch_);
        } while (safe_to_reclaim_epoch_ > safe_to_reclaim_epoch);
    }

    bool is_safely_reclaimable(uint64_t epoch) const { return (epoch <= safe_to_reclaim_epoch); }

    /// CPR checkpoint functions.
    inline void reset_phase_finished() {
        for (uint32_t idx = 1; idx <= _num_entries; ++idx) {
            auto phase = _table[idx].phase_finished.load();
            ////RDMS_ASSERT2(phase == StateBackendPhase::Rest || phase == StateBackendPhase::ResetInProgress, "Invalid phase {}",
            //                         to_string(phase));
            _table[idx].phase_finished.store(StateBackendPhase::Rest);
        }
    }
    /// This thread has completed the specified phase.
    inline bool finish_thread_phase(StateBackendPhase phase) {
        uint32_t entry = Runtime::NesThread::getId();
        _table[entry].phase_finished = phase;
        // Check if other threads have reported complete.
        for (uint32_t idx = 1; idx <= _num_entries; ++idx) {
            StateBackendPhase entry_phase = _table[idx].phase_finished.load();
            uint64_t entry_epoch = _table[idx].local_current_epoch;
            if (entry_epoch != 0 && entry_phase != phase) {
                return false;
            }
        }
        return true;
    }
    /// Has this thread completed the specified phase (i.e., is it waiting for other threads to
    /// finish the specified phase, before it can advance the global phase)?
    inline bool has_thread_finished_phase(StateBackendPhase phase) const {
        uint32_t entry = Runtime::NesThread::getId();
        return _table[entry].phase_finished == phase;
    }
};

//// page allocator

/// Address into a fixed page.
struct FixedPageAddress {
    static constexpr uint64_t kInvalidAddress = 0;

    /// A fixed-page address is 8 bytes.
    /// --of which 48 bits are used for the address. (The remaining 16 bits are used by the hash
    /// table, for control bits and the tag.)
    static constexpr uint64_t kAddressBits = 48;
    static constexpr uint64_t kMaxAddress = ((uint64_t) 1 << kAddressBits) - 1;

    /// --of which 20 bits are used for offsets into a page, of size 2^20 = 1 million items.
    static constexpr uint64_t kOffsetBits = 20;
    static constexpr uint64_t kMaxOffset = ((uint64_t) 1 << kOffsetBits) - 1;

    /// --and the remaining 28 bits are used for the page index, allowing for approximately 256
    /// million pages.
    static constexpr uint64_t kPageBits = kAddressBits - kOffsetBits;
    static constexpr uint64_t kMaxPage = ((uint64_t) 1 << kPageBits) - 1;

    FixedPageAddress() : _control{0} {}

    FixedPageAddress(uint64_t control) : _control{control} {}

    friend bool operator==(const FixedPageAddress& lhs, const FixedPageAddress& rhs) { return lhs._control == rhs._control; }
    friend bool operator<(const FixedPageAddress& lhs, const FixedPageAddress& rhs) { return lhs._control < rhs._control; }

    friend bool operator>(const FixedPageAddress& lhs, const FixedPageAddress& rhs) { return lhs._control > rhs._control; }

    friend bool operator>=(const FixedPageAddress& lhs, const FixedPageAddress& rhs) { return lhs._control >= rhs._control; }

    FixedPageAddress operator++() { return FixedPageAddress(++_control); }

    uint32_t offset() const { return static_cast<uint32_t>(_offset); }

    uint64_t page() const { return _page; }

    uint64_t control() const { return _control; }

    template<typename ostream_type>
    friend ostream_type& operator<<(ostream_type& stream, const FixedPageAddress& self) {
        return stream << "FixedPageAddress{offset: " << self.offset() << " page: " << self.page() << "}";
    }

    union {
        struct {
            uint64_t _offset : kOffsetBits;        // 20 bits
            uint64_t _page : kPageBits;            // 28 bits
            uint64_t _reserved : 64 - kAddressBits;// 16 bits
        };
        uint64_t _control;
    };
};
static_assert(sizeof(FixedPageAddress) == 8, "sizeof(FixedPageAddress) != 8");

/// Atomic address into a fixed page.
class AtomicFixedPageAddress {
  public:
    AtomicFixedPageAddress(const FixedPageAddress& address) : _control(address._control) {}

    /// Atomic access.
    inline FixedPageAddress load() const { return FixedPageAddress(_control.load()); }

    void store(FixedPageAddress value) { _control.store(value._control); }

    FixedPageAddress operator++(int) { return FixedPageAddress(_control++); }

  private:
    /// Atomic access to the address.
    std::atomic<uint64_t> _control;
};
static_assert(sizeof(AtomicFixedPageAddress) == 8, "sizeof(AtomicFixedPageAddress) != 8");

struct FreeAddress {
    FixedPageAddress removed_addr;
    uint64_t removal_epoch;
};

template<typename T>
class fixed_page {
  public:
    static constexpr uint64_t kPageSize = FixedPageAddress::kMaxOffset + 1;

    /// Accessors.
    inline const T& element(uint32_t offset) const {
        //        ////////RDMS_VERIFY2(offset <= FixedPageAddress::kMaxOffset, "Wrong offset {}", offset);
        return _elements[offset];
    }
    inline T& element(uint32_t offset) {
        //        ////////RDMS_VERIFY2(offset <= FixedPageAddress::kMaxOffset, "Wrong offset {}", offset);
        return _elements[offset];
    }

  private:
    /// The page's contents.
    T _elements[kPageSize];
    static_assert(alignof(T) <= 64, "alignof(item_t) > 64");
};

template<typename T>
class FixedPageArray {
  public:
    typedef T item_t;
    typedef fixed_page<T> page_t;
    typedef FixedPageArray<T> array_t;

  protected:
    FixedPageArray(uint64_t alignment_, uint64_t size_, const array_t* old_array) : alignment(alignment_), size(size_) {
        //        ////////RDMS_VERIFY2(is_pow_of_two(size), "size {} is not pow of two", size);
        uint64_t idx = 0;
        if (old_array) {
            ////RDMS_ASSERT2(old_array->size < size, "Old array is not smaller than size {} {}", old_array->size, size);
            for (; idx < old_array->size; ++idx) {
                page_t* page;
                page = old_array->pages()[idx].load(std::memory_order_acquire);
                while (page == nullptr) {
                    std::this_thread::yield();
                    page = old_array->pages()[idx].load(std::memory_order_acquire);
                }
                pages()[idx] = page;
            }
        }
        for (; idx < size; ++idx) {
            pages()[idx] = nullptr;
        }
    }

  public:
    static FixedPageArray<T>* create(uint64_t alignment, uint64_t size, const array_t* old_array) {
        auto* buffer = allocAligned(sizeof(array_t) + size * sizeof(std::atomic<page_t*>));
        return new (buffer) array_t(alignment, size, old_array);
    }

    static void destroy(array_t* arr, bool owns_pages) {
        NES_ASSERT(arr, "null array");
        if (owns_pages) {
            for (uint64_t idx = 0; idx < arr->size; ++idx) {
                page_t* page = arr->pages()[idx].load(std::memory_order_acquire);
                free(page);
            }
        }
        free(arr);
    }

    /// Used by allocator.Get().
    inline page_t* get(uint64_t page_idx) {
        //RDMS_ASSERT2(page_idx < size, "Page index {} not smaller than size {}", page_idx, size);
        return pages()[page_idx].load(std::memory_order_acquire);
    }

    /// Used by allocator.Allocate().
    inline page_t* get_or_add(uint64_t page_idx) {
        ////RDMS_VERIFY2(page_idx < size, "Invalid page idx {} {}", page_idx, size);
        page_t* page = pages()[page_idx].load(std::memory_order_acquire);
        while (page == nullptr) {
            page = add_page(page_idx);
        }
        return page;
    }

    inline page_t* add_page(uint64_t page_idx) {
        //////RDMS_VERIFY2(page_idx < size, "Invalid page idx {} {}", page_idx, size);
        void* buffer = allocAligned(sizeof(page_t), alignment);
        page_t* new_page = new (buffer) page_t{};
        page_t* expected = nullptr;
        if (pages()[page_idx].compare_exchange_strong(expected, new_page, std::memory_order_release)) {
            return new_page;
        } else {
            free(new_page);
            return expected;
        }
    }

  private:
    /// Accessors, since zero-length arrays at the ends of structs aren't standard in C++.
    const std::atomic<page_t*>* pages() const { return reinterpret_cast<const std::atomic<page_t*>*>(this + 1); }
    std::atomic<page_t*>* pages() { return reinterpret_cast<std::atomic<page_t*>*>(this + 1); }

  public:
    /// Alignment at which each page is allocated.
    const uint64_t alignment;
    /// Maximum number of pages in the array; fixed at time of construction.
    const uint64_t size;
    /// Followed by [size] std::atomic<> pointers to (page_t) pages. (Not shown here.)
};

class alignas(64) PageFreeList {
  public:
    std::deque<FreeAddress> free_list;
};

template<typename T>
class FixedPageAllocator {
    typedef fixed_page<T> page_t;
    typedef FixedPageArray<T> array_t;
    typedef FixedPageAllocator<T> alloc_t;

  public:
    explicit FixedPageAllocator(uint64_t alignment, LightEpochManager& epoch) : _alignment(alignment), _count(0), _epoch(&epoch) {
        array_t* page_array = array_t::create(alignment, 32, nullptr);
        page_array->add_page(0);
        _page_array.store(page_array, std::memory_order_release);
    }

    ~FixedPageAllocator() {
        array_t* ptr;
        if ((ptr = _page_array.load()) != nullptr) {
            array_t::destroy(ptr, true);
        }
    }

    inline T& operator[](FixedPageAddress address) {
        page_t* page = _page_array.load(std::memory_order_acquire)->get(address.page());
        //RDMS_ASSERT2(page, "Invalid page at page {} offset {}", address.page(), address.offset());
        return page->element(address.offset());
    }

    inline const T& operator[](FixedPageAddress address) const {
        page_t* page = _page_array.load(std::memory_order_acquire)->get(address.page());
        //RDMS_ASSERT2(page, "Invalid page at page {} offset {}", address.page(), address.offset());
        return page->element(address.offset());
    }

    std::deque<FreeAddress>& free_list() { return _free_list[Runtime::NesThread::getId()].free_list; }

    const std::deque<FreeAddress>& free_list() const { return _free_list[Runtime::NesThread::getId()].free_list; }

    FixedPageAddress count() const { return _count.load(); }

    void preallocate() {
        for (auto i = 0; i < 8; ++i) {
            free_at_epoch(allocate(), 0);
        }
    }

  private:
    FixedPageArray<T>* expand(array_t* expected, uint64_t new_size) {
        class DeleteContext : public AsyncContextBase {
          public:
            DeleteContext(array_t* arr_) : vec{arr_} {}
            /// The deep-copy constructor.
            DeleteContext(const DeleteContext& other) : vec{other.vec} {}

          protected:
            AsyncStatus deep_copy_internal(AsyncContextBase*& context_copy) final {
                return AsyncContextBase::deep_copy_internal(*this, context_copy);
            }

          public:
            array_t* vec;
        };

        auto delete_callback = [](AsyncContextBase* ctxt) {
            CallbackContext<DeleteContext> context{ctxt};
            array_t::destroy(context->vec, false);
        };

        //RDMS_ASSERT2(is_pow_of_two(new_size), "Size {} not pow of two", new_size);

        do {
            array_t* new_array = array_t::create(_alignment, new_size, expected);
            if (_page_array.compare_exchange_strong(expected, new_array, std::memory_order_release)) {
                // Have to free the old array, under epoch protection.
                DeleteContext context{expected};
                AsyncContextBase* context_copy;
                AsyncStatus result = context.deep_copy(context_copy);
                //                RDMS_ASSERT(result == AsyncStatus::ok, "wrong result");
                _epoch->bump_epoch(delete_callback, context_copy);
                return new_array;
            } else {
                free(new_array);
            }
        } while (expected->size < new_size);
        return expected;
    }

  public:
    FixedPageAddress allocate() {
        auto& q = free_list();
        if (!q.empty()) {
            if (q.front().removal_epoch <= _epoch->safe_to_reclaim_epoch.load()) {
                FixedPageAddress removed_addr = q.front().removed_addr;
                q.pop_front();
                return removed_addr;
            }
        }
        // Determine insertion page_index.
        FixedPageAddress addr = _count++;
        auto* page_array = _page_array.load(std::memory_order_acquire);
        if (addr.page() >= page_array->size) {
            // Need to resize the page array.
            page_array = expand(page_array, next_power_of_two(addr.page() + 1));
        }
        if (addr.offset() == 0 && addr.page() + 1 < page_array->size) {
            // Add the next page early, to try to avoid blocking other threads.
            page_array->add_page(addr.page() + 1);
        }
        page_array->get_or_add(addr.page());
        return addr;
    }

    void free_at_epoch(FixedPageAddress addr, uint64_t removed_epoch) { free_list().push_back(FreeAddress{addr, removed_epoch}); }

  private:
    /// Alignment at which each page is allocated.
    uint64_t _alignment;
    /// Array of all of the pages we've allocated.
    std::atomic<array_t*> _page_array;
    /// How many elements we've allocated.
    AtomicFixedPageAddress _count;

    LightEpochManager* _epoch;
    PageFreeList _free_list[Runtime::NesThread::MaxNumThreads];
};

//// hash index

class AtomicHashBucketEntry;

class HashBucketEntry {
    friend class AtomicHashBucketEntry;

  public:
    static constexpr uint64_t invalid_entry = 0;

    HashBucketEntry(uint64_t control_code = invalid_entry) : _control(control_code) {
        // nop
    }

    HashBucketEntry(uint64_t address, uint16_t tag, bool tentative) : _address(address), _tag(tag), _tentative(tentative) {
        // nop
    }

    HashBucketEntry(const HashBucketEntry& other) : _control(other._control) {
        // nop
    }

    HashBucketEntry& operator=(const HashBucketEntry& other) {
        _control = other._control;
        return *this;
    }

    friend bool operator==(const HashBucketEntry& lhs, const HashBucketEntry& rhs) { return lhs._control == rhs._control; }

    friend bool operator!=(const HashBucketEntry& lhs, const HashBucketEntry& rhs) { return lhs._control != rhs._control; }

    inline bool unused() const { return _control == 0; }

    //    inline TaggedAddress address() const { return TaggedAddress(_address); }

    inline TaggedAddress address() const { return _address; }

    inline uint16_t tag() const { return static_cast<uint16_t>(_tag); }

    inline bool tentative() const { return static_cast<bool>(_tentative); }

    inline void set_tentative(bool desired) { _tentative = desired; }

  private:
    union {
        struct {
            uint64_t _address : 48;// corresponds to logical address
            uint64_t _tag : 14;
            uint64_t _reserved : 1;
            uint64_t _tentative : 1;
        };
        uint64_t _control{invalid_entry};
    };
};
static_assert(sizeof(HashBucketEntry) == 8, "sizeof(HashBucketEntry) != 8");

class AtomicHashBucketEntry {
  public:
    AtomicHashBucketEntry() : _control(HashBucketEntry::invalid_entry) {
        // nop
    }

    AtomicHashBucketEntry(const HashBucketEntry& entry) : _control(entry._control) {
        // nop
    }

    inline HashBucketEntry load() const { return HashBucketEntry(_control.load()); }

    inline void store(const HashBucketEntry& desired) { _control.store(desired._control); }

    inline bool compare_exchange_strong(HashBucketEntry& expected, HashBucketEntry desired) {
        uint64_t expected_control = expected._control;
        bool result = _control.compare_exchange_strong(expected_control, desired._control);
        expected = HashBucketEntry(expected_control);
        return result;
    }

  private:
    std::atomic<uint64_t> _control;
};

class HashBucketOverflowEntry {
  public:
    HashBucketOverflowEntry() : _control(0) {
        // nop
    }

    HashBucketOverflowEntry(uint64_t address) : _address(address), _unused(0) {
        // nop
    }

    HashBucketOverflowEntry(FixedPageAddress address) : _address(address.control()), _unused(0) {
        // nop
    }

    HashBucketOverflowEntry(const HashBucketOverflowEntry& other) : _control(other._control) {
        // nop
    }

    inline HashBucketOverflowEntry& operator=(const HashBucketOverflowEntry& other) {
        _control = other._control;
        return *this;
    }

    friend bool operator==(const HashBucketOverflowEntry& lhs, const HashBucketOverflowEntry& rhs) {
        return lhs._control == rhs._control;
    }

    friend bool operator!=(const HashBucketOverflowEntry& lhs, const HashBucketOverflowEntry& rhs) {
        return lhs._control != rhs._control;
    }

    inline bool unused() const { return _address == 0; }

    inline uint64_t address() const { return (_address); }

    union {
        struct {
            uint64_t _address : 48;// corresponds to logical address
            uint64_t _unused : 16;
        };
        uint64_t _control;
    };
};
static_assert(sizeof(HashBucketOverflowEntry) == 8, "sizeof(HashBucketOverflowEntry) != 8");

class AtomicHashBucketOverflowEntry {
  private:
    static constexpr uint64_t pin_increment = (uint64_t) 1 << 48;
    static constexpr uint64_t locked = (uint64_t) 1 << 63;

  public:
    AtomicHashBucketOverflowEntry(const AtomicHashBucketOverflowEntry& entry) { _control.store(entry._control); }

    AtomicHashBucketOverflowEntry() : _control(HashBucketEntry::invalid_entry) {
        // nop
    }

    AtomicHashBucketOverflowEntry& operator=(const AtomicHashBucketOverflowEntry& entry) {
        _control.store(entry._control);
        return *this;
    }

    /// Atomic access.
    inline HashBucketOverflowEntry load() const { return HashBucketOverflowEntry(_control.load()); }
    inline void store(const HashBucketOverflowEntry& desired) { _control.store(desired._control); }
    inline bool compare_exchange_strong(HashBucketOverflowEntry& expected, HashBucketOverflowEntry desired) {
        uint64_t expected_control = expected._control;
        bool result = _control.compare_exchange_strong(expected_control, desired._control);
        expected = HashBucketOverflowEntry(expected_control);
        return result;
    }

  private:
    /// Atomic address to the hash bucket entry.
    std::atomic<uint64_t> _control;
};

/// A bucket consisting of 7 hash bucket entries, plus one hash bucket overflow entry. Fits in
/// a cache line.
struct alignas(64) HashBucket {
    /// Number of entries per bucket (excluding overflow entry).
    static constexpr uint32_t num_entries = 7;
    /// The entries.
    AtomicHashBucketEntry entries[num_entries];
    /// Overflow entry points to next overflow bucket, if any.
    AtomicHashBucketOverflowEntry overflow_entry;
};
static_assert(sizeof(HashBucket) == 64, "sizeof(HashBucket) != Constants::kCacheLineBytes");

struct KeyHash {
    KeyHash() : _control(0) {
        // nop
    }

    explicit KeyHash(uint64_t code) : _control(code) {
        // nop
    }

    KeyHash(const KeyHash& other) : _control(other._control) {
        // nop
    }

    KeyHash& operator=(const KeyHash& other) {
        _control = other._control;
        return *this;
    }

    /// Truncate the key hash's address to get the page_index into a hash table of specified size.
    inline uint64_t idx(uint64_t size) const {
        //        //////RDMS_VERIFY2(is_pow_of_two(size), "size {} is not pow of 2", size);
        return _address & (size - 1);
    }

    /// The tag (14 bits) serves as a discriminator inside a hash bucket. (Hash buckets use 2 bits
    /// for control and 48 bits for log-structured store offset; the remaining 14 bits discriminate
    /// between different key hashes stored in the same bucket.)
    inline uint16_t tag() const { return static_cast<uint16_t>(_tag); }

  public:
    union {
        struct {
            uint64_t _address : 48;
            uint64_t _tag : 14;
            uint64_t _not_used : 2;
        };
        uint64_t _control;
    };
};
static_assert(sizeof(KeyHash) == 8, "sizeof(KeyHash) != 8");

class alignas(64) HashIndex {
  public:
    explicit HashIndex(size_t size, Runtime::HardwareManagerPtr hardwareManager) : size(size), hardwareManager(hardwareManager) {
        buckets = static_cast<HashBucket*>(this->hardwareManager->getGlobalAllocator()->allocate(size * sizeof(HashBucket), 64));
        memset(buckets, 0, size * sizeof(HashBucket));
    }

    ~HashIndex() { hardwareManager->getGlobalAllocator()->deallocate(buckets, size, 64); }

    inline const HashBucket& bucket(KeyHash hash) const { return buckets[hash.idx(size)]; }

    inline HashBucket& bucket(KeyHash hash) { return buckets[hash.idx(size)]; }

    /// Get the bucket specified by the index. (Used by checkpoint/recovery.)
    inline const HashBucket& bucket(uint64_t idx) const {
        //        //////RDMS_VERIFY2(idx < _size, "Cannot find hash_bucket {}<{}", idx, _size);
        return buckets[idx];
    }

    /// (Used by GC and called by unit tests.)
    inline HashBucket& bucket(uint64_t idx) {
        //        //////RDMS_VERIFY2(idx < _size, "Cannot find hash_bucket {}<{}", idx, _size);
        return buckets[idx];
    }

  private:
    size_t size;
    HashBucket* buckets{nullptr};
    Runtime::HardwareManagerPtr hardwareManager;
};

template<typename key_type>
struct HasKeyHasher {
    static constexpr auto value = false;
};

template<typename key_type>
struct KeyHasher {
    static KeyHash hash(key_type) {
        static_assert(HasKeyHasher<key_type>::value, "this must not be compiled");
        return KeyHash();
    }
};

template<>
struct HasKeyHasher<uint64_t> {
    static constexpr auto value = true;
};

template<>
struct KeyHasher<uint64_t> {
    static KeyHash hash(uint64_t value) {
#if 0
/*
//      gcc 10 O3 skylake
mov     rax, rdi
movzx   ecx, si
imul    rcx, rcx, 40343
mov     edx, esi
shr     edx, 16
add     rdx, rcx
imul    rcx, rdx, 40343
mov     rdx, rsi
shr     rdx, 32
movzx   edx, dx
add     rdx, rcx
imul    rcx, rdx, 40343
shr     rsi, 48
add     rsi, rcx
imul    rcx, rsi, 40343
movabs  rdx, -2050514406865515592
add     rdx, rcx
rorx    rcx, rdx, 43
mov     qword ptr [rdi], rcx
ret
*/
uint64_t local_rand_hash = 8;
local_rand_hash = 40343ul * local_rand_hash + ((value)&0xFFFF);
local_rand_hash = 40343ul * local_rand_hash + ((value >> 16) & 0xFFFF);
local_rand_hash = 40343ul * local_rand_hash + ((value >> 32) & 0xFFFF);
local_rand_hash = 40343ul * local_rand_hash + (value >> 48);
local_rand_hash = 40343ul * local_rand_hash;
return KeyHash(ROTR64(local_rand_hash, 43ul));
#else
        /*
//      gcc 10 O3 skylake
        mov     rax, rdi
        mov     rcx, rsi
        shr     rcx, 33
        xor     rcx, rsi
        movabs  rdx, -49064778989728563
        imul    rdx, rcx
        mov     rcx, rdx
        shr     rcx, 33
        xor     rcx, rdx
        movabs  rdx, -4265267296055464877
        imul    rdx, rcx
        mov     rcx, rdx
        shr     rcx, 33
        xor     rcx, rdx
        mov     qword ptr [rdi], rcx
        ret
*/
        value ^= value >> 33;
        value *= 0xff51afd7ed558ccd;
        value ^= value >> 33;
        value *= 0xc4ceb9fe1a85ec53;
        value ^= value >> 33;
        return KeyHash(value);
#endif
    }
};

/// actual join state

class JoinState {
  public:
    using KeyValueRecord = KVRecord<uint64_t, JoinStateChunk>;

  private:
    static constexpr uint64_t invalid_address = 1;

  public:
    explicit JoinState(Runtime::HardwareManagerPtr hardwareManager, size_t indexSize, size_t bufferSize)
        : hardwareManager(std::move(hardwareManager)), bufferSize(bufferSize), index(indexSize, this->hardwareManager),
          overflowBucketsAllocator(64, epoch) {

        circularBuffer = static_cast<uint8_t*>(this->hardwareManager->getGlobalAllocator()->allocate(bufferSize, 64));
        tailOffset.store(reinterpret_cast<uintptr_t>(circularBuffer));
        headAddress = reinterpret_cast<uintptr_t>(circularBuffer);
        overrunAddress = reinterpret_cast<uintptr_t>(circularBuffer) + bufferSize;
    }

    ~JoinState() { hardwareManager->getGlobalAllocator()->deallocate(circularBuffer, bufferSize, 64); }

    template<typename V>
    bool rmw_atomic(KeyValueRecord* kv, V* value) {
        auto& chunk = kv->value();
        auto pos = chunk.pos.load();
        if (pos + sizeof(V) > chunk.data.size()) {
            return false;
        }
        auto writeIndex = chunk.pos.fetch_add(sizeof(V));
        auto* ptr = reinterpret_cast<V*>(chunk.data.begin() + writeIndex);
        *ptr = *value;
        return true;
    }

    template<typename V>
    bool rmw_create(KeyValueRecord* kv, V* value) {
        auto& chunk = kv->value();
        auto writeIndex = chunk.pos.fetch_add(sizeof(V));
        auto* ptr = reinterpret_cast<V*>(chunk.data.begin() + writeIndex);
        *ptr = *value;
        return true;
    }

    template<typename T>
    KeyValueRecord* rmw(uint64_t key, T* value) {
        KeyHash hash = KeyHasher<uint64_t>::hash(key);
        HashBucketEntry entry;
        AtomicHashBucketEntry* atomic_entry = find_create_entry(hash, entry);
        auto address = entry.address();
        if (address > headAddress) {
            auto* record = reinterpret_cast<KeyValueRecord*>(address.pointer());
            if (key != record->key()) {
                address = traceKey(key, record->header.previous_address(), overrunAddress);
            }
            // Mutable region. Try to update in place.
            if (atomic_entry->load() != entry) {
                // Some other thread may have RCUed the record before we locked it; try again.
                return rmw(key, value);
            }
            if (address > headAddress) {
                record = reinterpret_cast<KeyValueRecord*>(address.pointer());
                if (!record->header._tombstone && rmw_atomic(record, value)) {
                    return record;
                } else {
                    goto create_record;
                }
            }
        }
    create_record:
        const KeyValueRecord* old_record = reinterpret_cast<const KeyValueRecord*>(address.pointer());
        if (old_record->header._tombstone) {
            old_record = nullptr;
        }
        uint32_t recordSize = KeyValueRecord::size(sizeof(uint64_t), sizeof(JoinStateChunk));
        TaggedAddress newAddress = {tailOffset.fetch_add(recordSize)};
        auto* newKVRecord = reinterpret_cast<KeyValueRecord*>(newAddress.address());
        new (newKVRecord) KeyValueRecord(RecordHeader(true, false, false, entry.address()), key);
        rmw_create(newKVRecord, value);
        auto updated_entry = HashBucketEntry(newAddress.address(), hash.tag(), false);
        if (!atomic_entry->compare_exchange_strong(entry, updated_entry)) {
            // Try again.
            newKVRecord->header._invalid = true;
            return rmw(key, value);
        }
        return newKVRecord;
    }

  private:
    TaggedAddress traceKey(const uint64_t& key, TaggedAddress from_address_ptr, const TaggedAddress min_offset) {
        while
            PLACEHOLDER_LIKELY(from_address_ptr >= min_offset) {
                //            //////RDMS_VERIFY2(from_address_ptr < hlog.overrun_address() &&
                //                             from_address_ptr >= hlog.head_address(), "invalid header prev address {}<{}<{}", hlog.head_address(), from_address_ptr, hlog.overrun_address());
                const auto* record = reinterpret_cast<const KeyValueRecord*>(from_address_ptr.address());
                if (key == record->key()) {
                    return from_address_ptr;
                } else {
#ifdef ENABLE_CYCLE_CHECK
                    //RDMS_ASSERT2(seen.find(from_address_ptr.address()) == seen.end(),
                     "Addr {} already seen in {} - cycle!",
                     from_address_ptr.address(),
                     seen.size());
                     seen.emplace(from_address_ptr.address());
                     //                //RDMS_ASSERT2(record->header.previous_address() < from_address_ptr, "Prev addr {} must be smaller than {}!",
                     //                             record->header.previous_address().control(),
                     //                             from_address_ptr.address());
#endif
                     from_address_ptr = record->header.previous_address();
                     continue;
                }
            }
        return from_address_ptr;
    }

    bool has_conflicting_entry(const HashBucket* bucket, const AtomicHashBucketEntry* atomic_entry) const {
        uint16_t tag = atomic_entry->load().tag();
        while (true) {
            for (uint32_t entry_idx = 0; entry_idx < HashBucket::num_entries; ++entry_idx) {
                HashBucketEntry entry = bucket->entries[entry_idx].load();
                if PLACEHOLDER_UNLIKELY (entry != HashBucketEntry::invalid_entry && entry.tag() == tag
                                         && atomic_entry != &bucket->entries[entry_idx]) {
                    // Found a conflict.
                    return true;
                }
            }
            // Go to next bucket in the chain
            HashBucketOverflowEntry entry = bucket->overflow_entry.load();
            if PLACEHOLDER_LIKELY (entry.unused()) {
                // Reached the end of the bucket chain; no conflicts found.
                return false;
            }
            // Go to the next bucket.
            bucket = &overflowBucketsAllocator[entry.address()];
            //            //////RDMS_VERIFY2(reinterpret_cast<size_t>(bucket) % 64 == 0,
            //                         "Invalid bucket alignment {}", reinterpret_cast<size_t>(bucket));
        }
    }

    const AtomicHashBucketEntry* find_entry(KeyHash hash, HashBucketEntry& expected) {
        expected = HashBucketEntry::invalid_entry;
        const HashBucket* bucket = &index.bucket(hash);
        //        //////RDMS_VERIFY2(reinterpret_cast<size_t>(bucket) % 64 == 0,
        //                     "Invalid bucket alignment {}", reinterpret_cast<size_t>(bucket));
        while (true) {
            for (uint32_t entry_idx = 0; entry_idx < HashBucket::num_entries; ++entry_idx) {
                HashBucketEntry entry = bucket->entries[entry_idx].load();
                if PLACEHOLDER_UNLIKELY (entry.unused()) {
                    continue;
                }
                if (hash.tag() == entry.tag()) {
                    // Found a matching tag. (So, the input hash matches the entry on 14 tag bits +
                    // log_2(table size) address bits.)
                    if (!entry.tentative()) {
                        // If (final key, return immediately)
                        expected = entry;
                        return &bucket->entries[entry_idx];
                    }
                }
            }
            // Go to next bucket in the chain
            HashBucketOverflowEntry overflow_entry = bucket->overflow_entry.load();
            if (overflow_entry.unused()) {
                // No more buckets in the chain.
                return nullptr;
            }
            bucket = &overflowBucketsAllocator[overflow_entry.address()];
            //            //////RDMS_VERIFY2(reinterpret_cast<size_t>(bucket) % 64 == 0,
            //                         "Invalid bucket alignment {}", reinterpret_cast<size_t>(bucket));
        }
        //        FATAL_ERROR("unreached code");
        return nullptr;// NOT REACHED
    }

    AtomicHashBucketEntry* find_create_entry(KeyHash hash, HashBucketEntry& expected) {
        while (true) {
            HashBucket* bucket = &index.bucket(hash);
            //            //////RDMS_VERIFY2(reinterpret_cast<size_t>(bucket) % 64 == 0,
            //                         "Invalid bucket alignment {}", reinterpret_cast<size_t>(bucket));

            AtomicHashBucketEntry* atomic_entry = find_tentative_entry(hash, bucket, expected);
            if PLACEHOLDER_LIKELY (expected != HashBucketEntry::invalid_entry) {
                // Found an existing hash bucket entry; nothing further to check.
#ifdef STATE_WITH_STATISTICS
                find_create_match++;
#endif
                return atomic_entry;
            }
            // We have a free slot.
            // Try to install tentative tag in free slot.
            HashBucketEntry entry(invalid_address, hash.tag(), true);
            if PLACEHOLDER_LIKELY (atomic_entry->compare_exchange_strong(expected, entry)) {
                // See if some other thread is also trying to install this tag.
                if (has_conflicting_entry(bucket, atomic_entry)) {
                    // Back off and try again.
                    atomic_entry->store(HashBucketEntry::invalid_entry);
                } else {
                    // No other thread was trying to install this tag, so we can clear our entry's "tentative"
                    // bit.
                    expected = HashBucketEntry(invalid_address, hash.tag(), false);
                    atomic_entry->store(expected);
                    return atomic_entry;
                }
            }
        }
        //        FATAL_ERROR("unreached code");
        return nullptr;// NOT REACHED
    }

    AtomicHashBucketEntry* find_tentative_entry(KeyHash hash, HashBucket* bucket, HashBucketEntry& expected_entry) {
        expected_entry = HashBucketEntry::invalid_entry;
        AtomicHashBucketEntry* atomic_entry = nullptr;
        // Try to find a slot that contains the right tag or that's free.
        while (true) {
            // Search through the bucket looking for our key. Last entry is reserved
            // for the overflow pointer.
            for (uint32_t entry_idx = 0; entry_idx < HashBucket::num_entries; ++entry_idx) {
                HashBucketEntry entry = bucket->entries[entry_idx].load();
                if (entry.unused()) {
                    if (atomic_entry == nullptr) {
                        // Found a free slot; keep track of it, and continue looking for a match.
                        atomic_entry = &bucket->entries[entry_idx];
                    }
                    continue;
                }
                if PLACEHOLDER_LIKELY (hash.tag() == entry.tag() && !entry.tentative()) {
                    // Found a match. (So, the input hash matches the entry on 14 tag bits +
                    // log_2(table size) address bits.) Return it to caller.
                    expected_entry = entry;
                    return &bucket->entries[entry_idx];
                }
            }
            // Go to next bucket in the chain
            HashBucketOverflowEntry overflow_entry = bucket->overflow_entry.load();
            if PLACEHOLDER_UNLIKELY (overflow_entry.unused()) {
                // No more buckets in the chain.
                if (atomic_entry != nullptr) {
                    // We found a free slot earlier (possibly inside an earlier bucket).
                    return atomic_entry;
                }
                // We didn't find any free slots, so allocate new bucket.
                FixedPageAddress new_bucket_addr = overflowBucketsAllocator.allocate();
                bool success;
                do {
                    auto new_bucket_entry = HashBucketOverflowEntry(new_bucket_addr);
                    success = bucket->overflow_entry.compare_exchange_strong(overflow_entry, new_bucket_entry);
                } while (!success && overflow_entry.unused());
#ifdef STATE_WITH_STATISTICS
                find_create_overflow++;
#endif
                if (!success) {
                    // Install failed, undo allocation; use the winner's entry
                    overflowBucketsAllocator.free_at_epoch(new_bucket_addr, 0);
                } else {
                    // Install succeeded; we have a new bucket on the chain. Return its first slot.
                    bucket = &overflowBucketsAllocator[new_bucket_addr];
                    //                    RDMS_VERIFY(expected_entry == HashBucketEntry::invalid_entry, "invalid entry");
#ifdef STATE_WITH_STATISTICS
                    find_create_overflow_succeed++;
#endif
                    return &bucket->entries[0];
                }
            }
            // Go to the next bucket.
            bucket = &overflowBucketsAllocator[overflow_entry.address()];
            //            //////RDMS_VERIFY2(reinterpret_cast<size_t>(bucket) % 64 == 0,
            //                         "Invalid bucket alignment {}", reinterpret_cast<size_t>(bucket));
        }
        NES_ASSERT2_FMT(false, "unreached code");
        return nullptr;// NOT REACHED
    }

  private:
    Runtime::HardwareManagerPtr hardwareManager;
    size_t bufferSize;
    HashIndex index;
    uint8_t* circularBuffer{nullptr};
    std::atomic<uint64_t> tailOffset{0};
    uint64_t headAddress;
    uint64_t overrunAddress;

    LightEpochManager epoch;
    FixedPageAllocator<HashBucket> overflowBucketsAllocator;
};

using JoinStatePtr = std::shared_ptr<JoinState>;

#ifndef min
#define min(A, B) ((A) < (B) ? (A) : (B))
#endif

int memcpy_uncached_store_avx(void* dest, const void* src, size_t n_bytes) {
    int ret = 0;
#ifdef __AVX__
    char* d = (char*) dest;
    uintptr_t d_int = (uintptr_t) d;
    const char* s = (const char*) src;
    uintptr_t s_int = (uintptr_t) s;
    size_t n = n_bytes;

    // align dest to 256-bits
    if (d_int & 0x1f) {
        size_t nh = min(0x20 - (d_int & 0x1f), n);
        memcpy(d, s, nh);
        d += nh;
        d_int += nh;
        s += nh;
        s_int += nh;
        n -= nh;
    }

    if (s_int & 0x1f) {// src is not aligned to 256-bits
        __m256d r0, r1, r2, r3;
        // unroll 4
        while (n >= 4 * sizeof(__m256d)) {
            r0 = _mm256_loadu_pd((double*) (s + 0 * sizeof(__m256d)));
            r1 = _mm256_loadu_pd((double*) (s + 1 * sizeof(__m256d)));
            r2 = _mm256_loadu_pd((double*) (s + 2 * sizeof(__m256d)));
            r3 = _mm256_loadu_pd((double*) (s + 3 * sizeof(__m256d)));
            _mm256_stream_pd((double*) (d + 0 * sizeof(__m256d)), r0);
            _mm256_stream_pd((double*) (d + 1 * sizeof(__m256d)), r1);
            _mm256_stream_pd((double*) (d + 2 * sizeof(__m256d)), r2);
            _mm256_stream_pd((double*) (d + 3 * sizeof(__m256d)), r3);
            s += 4 * sizeof(__m256d);
            d += 4 * sizeof(__m256d);
            n -= 4 * sizeof(__m256d);
        }
        while (n >= sizeof(__m256d)) {
            r0 = _mm256_loadu_pd((double*) (s));
            _mm256_stream_pd((double*) (d), r0);
            s += sizeof(__m256d);
            d += sizeof(__m256d);
            n -= sizeof(__m256d);
        }
    } else {// or it IS aligned
        __m256d r0, r1, r2, r3, r4, r5, r6, r7;
        // unroll 8
        while (n >= 8 * sizeof(__m256d)) {
            r0 = _mm256_load_pd((double*) (s + 0 * sizeof(__m256d)));
            r1 = _mm256_load_pd((double*) (s + 1 * sizeof(__m256d)));
            r2 = _mm256_load_pd((double*) (s + 2 * sizeof(__m256d)));
            r3 = _mm256_load_pd((double*) (s + 3 * sizeof(__m256d)));
            r4 = _mm256_load_pd((double*) (s + 4 * sizeof(__m256d)));
            r5 = _mm256_load_pd((double*) (s + 5 * sizeof(__m256d)));
            r6 = _mm256_load_pd((double*) (s + 6 * sizeof(__m256d)));
            r7 = _mm256_load_pd((double*) (s + 7 * sizeof(__m256d)));
            _mm256_stream_pd((double*) (d + 0 * sizeof(__m256d)), r0);
            _mm256_stream_pd((double*) (d + 1 * sizeof(__m256d)), r1);
            _mm256_stream_pd((double*) (d + 2 * sizeof(__m256d)), r2);
            _mm256_stream_pd((double*) (d + 3 * sizeof(__m256d)), r3);
            _mm256_stream_pd((double*) (d + 4 * sizeof(__m256d)), r4);
            _mm256_stream_pd((double*) (d + 5 * sizeof(__m256d)), r5);
            _mm256_stream_pd((double*) (d + 6 * sizeof(__m256d)), r6);
            _mm256_stream_pd((double*) (d + 7 * sizeof(__m256d)), r7);
            s += 8 * sizeof(__m256d);
            d += 8 * sizeof(__m256d);
            n -= 8 * sizeof(__m256d);
        }
        while (n >= sizeof(__m256d)) {
            r0 = _mm256_load_pd((double*) (s));
            _mm256_stream_pd((double*) (d), r0);
            s += sizeof(__m256d);
            d += sizeof(__m256d);
            n -= sizeof(__m256d);
        }
    }

    if (n)
        memcpy(d, s, n);

    // fencing is needed even for plain memcpy(), due to performance
    // being hit by delayed flushing of WC buffers
    _mm_sfence();

#else
#error "this file should be compiled with -mavx"
#endif
    return ret;
}

template<typename T>
class JoinPipelineStage : public Runtime::Execution::ExecutablePipelineStage {
  public:
    explicit JoinPipelineStage(PipelineStageArity arity)
        : Runtime::Execution::ExecutablePipelineStage(arity) {}//, joinState(std::move(joinState)) {}

    ExecutionResult
    execute(Runtime::TupleBuffer& buffer, Runtime::Execution::PipelineExecutionContext&, Runtime::WorkerContext& wctx) override {
        auto* input = buffer.getBuffer<T>();
        auto numOfTuples = buffer.getNumberOfTuples();

        //        auto& localHt = cache[wctx.getId()];

        for (uint32_t i = 0; i < numOfTuples; ++i) {
            auto* record = input + i;
           
        }

        return ExecutionResult::Ok;
    }

    void reconfigure(Runtime::ReconfigurationMessage& message, Runtime::WorkerContext& context) override {
        Reconfigurable::reconfigure(message, context);
        switch (message.getType()) {
            case Runtime::Initialize: {
                cache[context.getId()].data = new T[1'000'000];
                index[context.getId()].index = 0;
                break;
            }
            case Runtime::HardEndOfStream:
            case Runtime::SoftEndOfStream: {
                delete[] cache[context.getId()].data;
                cache[context.getId()].data = nullptr;
                break;
            }
            default: break;
        }
    }

  private:
    //    using HashTableType = tsl::robin_map<uint64_t, JoinState::KeyValueRecord*>;
    //    std::array<HashTableType, Runtime::NesThread::MaxNumThreads> cache;
    //    JoinStatePtr joinState;

    struct alignas(64) Index {
        uint64_t index;
    };

    struct alignas(64) Data {
        T* data;
    };

    std::array<Data, Runtime::NesThread::MaxNumThreads> cache;
    std::array<Index, Runtime::NesThread::MaxNumThreads> index;
};

class LeftJoinPipelineStage : public JoinPipelineStage<LeftStream> {
  public:
    explicit LeftJoinPipelineStage() : JoinPipelineStage(BinaryLeft) {}
};

class RightJoinPipelineStage : public JoinPipelineStage<RightStream> {
  public:
    explicit RightJoinPipelineStage() : JoinPipelineStage(BinaryRight) {}
};

void createBenchmarkSources(std::vector<DataSourcePtr>& dataProducers,
                            Runtime::NodeEnginePtr nodeEngine,
                            SchemaPtr schema,
                            size_t partitionSizeBytes,
                            uint64_t numOfSources,
                            uint64_t startOffset,
                            Runtime::Execution::ExecutablePipelinePtr pipeline,
                            std::function<void(uint8_t*, size_t)> writer) {
    auto ofs = dataProducers.size() + startOffset;
    auto bufferSize = nodeEngine->getBufferManager()->getBufferSize();
    size_t numOfBuffers = partitionSizeBytes / bufferSize;
    for (auto i = 0ul; i < numOfSources; ++i) {
        size_t dataSegmentSize = bufferSize * numOfBuffers;
        auto* data = new uint8_t[dataSegmentSize];
        writer(data, dataSegmentSize);
        std::shared_ptr<uint8_t> ptr(data, [](uint8_t* ptr) {
            delete[] ptr;
        });
        std::vector<Runtime::Execution::SuccessorExecutablePipeline> pipelines;
        pipelines.emplace_back(pipeline);
        auto source = std::make_shared<BenchmarkSource>(schema,
                                                        std::move(ptr),
                                                        dataSegmentSize,
                                                        nodeEngine->getBufferManager(),
                                                        nodeEngine->getQueryManager(),
                                                        numOfBuffers,
                                                        0ul,
                                                        i + ofs,
                                                        64ul,
                                                        GatheringMode::FREQUENCY_MODE,
                                                        SourceMode::COPY_BUFFER,
                                                        i,
                                                        0ul,
                                                        pipelines);
        dataProducers.emplace_back(source);
    }
}
}// namespace NES
int main() {
    using namespace NES;
    NES::setupLogging("LazyJoin.log", NES::LOG_INFO);
    auto lhsStreamSchema = Schema::create()
                               ->addField("key", DataTypeFactory::createUInt64())
                               ->addField("timestamp", DataTypeFactory::createUInt64());

    auto rhsStreamSchema = Schema::create()
                               ->addField("key", DataTypeFactory::createUInt64())
                               ->addField("timestamp", DataTypeFactory::createUInt64());

    std::vector<PhysicalSourcePtr> physicalSources;

    auto engine = Runtime::NodeEngineFactory::createNodeEngine("127.0.0.1",
                                                               0,
                                                               physicalSources,
                                                               8,
                                                               128 * 1024,
                                                               1024,
                                                               64,
                                                               64,
                                                               Configurations::QueryCompilerConfiguration());

    auto sink = createNullOutputSink(0, engine);

    std::vector<Runtime::Execution::OperatorHandlerPtr> operatorHandlers;

    //    auto joinStateLeft = std::make_shared<JoinState>(engine->getHardwareManager(), 1024, 1024 * 1024 * 1024);
    //    auto joinStateRight = std::make_shared<JoinState>(engine->getHardwareManager(), 1024, 1024 * 1024 * 1024);

    auto executionContextLeft = std::make_shared<Runtime::Execution::PipelineExecutionContext>(
        0,
        engine->getQueryManager(),
        [sink](Runtime::TupleBuffer& buffer, Runtime::WorkerContext& wctx) {
            sink->writeData(buffer, wctx);
        },
        [](Runtime::TupleBuffer&) {
        },
        operatorHandlers);

    auto executionContextRight = std::make_shared<Runtime::Execution::PipelineExecutionContext>(
        0,
        engine->getQueryManager(),
        [sink](Runtime::TupleBuffer& buffer, Runtime::WorkerContext& wctx) {
            sink->writeData(buffer, wctx);
        },
        [](Runtime::TupleBuffer&) {
        },
        operatorHandlers);

    auto executableLeft = std::make_shared<LeftJoinPipelineStage>();
    auto executableRight = std::make_shared<RightJoinPipelineStage>();

    auto numSourcesLeft = 4;
    auto numSourcesRight = 4;

    auto pipelineLeft =
        Runtime::Execution::ExecutablePipeline::create(0, 0, executionContextLeft, executableLeft, numSourcesLeft, {sink});
    auto pipelineRight =
        Runtime::Execution::ExecutablePipeline::create(1, 0, executionContextRight, executableRight, numSourcesRight, {sink});

    std::vector<DataSourcePtr> dataProducers;

    createBenchmarkSources(dataProducers,
                           engine,
                           lhsStreamSchema,
                           8 * 1024 * 1024,
                           numSourcesLeft,
                           1,
                           pipelineLeft,
                           [](uint8_t* data, size_t length) {
                               auto* ptr = reinterpret_cast<LeftStream*>(data);
                               auto numOfTuples = length / sizeof(LeftStream);
                               std::mt19937 engine(std::chrono::high_resolution_clock::now().time_since_epoch().count());
                               Slash::ycsb_zipfian_generator generator(0, 100'000, 0.4);

                               for (size_t i = 0; i < numOfTuples; ++i) {
                                   ptr[i].key = generator(engine);
                                   ptr[i].timestamp = 0;
                               }
                           });

    createBenchmarkSources(dataProducers,
                           engine,
                           rhsStreamSchema,
                           8 * 1024 * 1024,
                           numSourcesRight,
                           1,
                           pipelineRight,
                           [](uint8_t* data, size_t length) {
                               auto* ptr = reinterpret_cast<RightStream*>(data);
                               auto numOfTuples = length / sizeof(RightStream);
                               std::mt19937 engine(std::chrono::high_resolution_clock::now().time_since_epoch().count());
                               Slash::ycsb_zipfian_generator generator(0, 100'000, 0.4);

                               for (size_t i = 0; i < numOfTuples; ++i) {
                                   ptr[i].key = generator(engine);
                                   ptr[i].timestamp = 0;
                               }
                           });

    auto executionPlan = Runtime::Execution::ExecutableQueryPlan::create(0,
                                                                         0,
                                                                         dataProducers,
                                                                         {sink},
                                                                         {pipelineLeft, pipelineRight},
                                                                         engine->getQueryManager(),
                                                                         engine->getBufferManager());

    NES_ASSERT(engine->registerQueryInNodeEngine(executionPlan), "Cannot register query");

    auto startTs = std::chrono::high_resolution_clock::now();

    NES_ASSERT(engine->startQuery(executionPlan->getQueryId()), "Cannot start query");

    while (engine->getQueryStatus(executionPlan->getQueryId()) == Runtime::Execution::ExecutableQueryPlanStatus::Running) {
        _mm_pause();
    }

    auto statistics = engine->getQueryManager()->getQueryStatistics(executionPlan->getQuerySubPlanId());
    auto nowTs = std::chrono::high_resolution_clock::now();
    auto elapsedNs = std::chrono::duration_cast<std::chrono::nanoseconds>(nowTs - startTs);
    double elapsedSec = elapsedNs.count() / 1'000'000'000.0;
    double MTuples = statistics->getProcessedTuple() / 1'000'000.0;
    auto throughputTuples = MTuples / elapsedSec;
    auto throughputMBytes = ((16ul * MTuples * 1'000'000.0) / elapsedSec) / (1024ul * 1024ul);
    NES_INFO("Processed: " << throughputTuples << " MTuple/sec - " << throughputMBytes << " MB/sec");

    NES_ASSERT(engine->undeployQuery(executionPlan->getQueryId()), "Cannot undeploy query");
    NES_ASSERT(engine->stop(), "Cannot stop query");

    //    auto query = Query::from("window1").joinWith(Query::from("window2")).where(Attribute("id1")).equalsTo(Attribute("id2")).window(TumblingWindow::of(EventTime(Attribute("timestamp")), Milliseconds(1000))));
    //
    //    TestHarness testHarness = TestHarness(query, restPort, rpcPort)
    //                                  .addLogicalSource("window1", windowSchema)
    //                                  .addLogicalSource("window2", window2Schema)
    //                                  .attachWorkerWithCSVSourceToCoordinator("window1", csvSourceType1)
    //                                  .attachWorkerWithCSVSourceToCoordinator("window1", csvSourceType1)
    //                                  .attachWorkerWithCSVSourceToCoordinator("window2", csvSourceType2)
    //                                  .attachWorkerWithCSVSourceToCoordinator("window2", csvSourceType2)
    //                                  .validate()
    //                                  .setupTopology();

    struct Output {
        int64_t start;
        int64_t end;
        int64_t key;
        int64_t win1;
        uint64_t id1;
        uint64_t timestamp1;
        int64_t win2;
        uint64_t id2;
        uint64_t timestamp2;

        // overload the == operator to check if two instances are the same
        bool operator==(Output const& rhs) const {
            return (start == rhs.start && end == rhs.end && key == rhs.key && win1 == rhs.win1 && id1 == rhs.id1
                    && timestamp1 == rhs.timestamp1 && win2 == rhs.win2 && id2 == rhs.id2 && timestamp2 == rhs.timestamp2);
        }
    };

    std::vector<Output> expectedOutput = {{1000, 2000, 4, 1, 4, 1002, 3, 4, 1102},    {1000, 2000, 4, 1, 4, 1002, 3, 4, 1112},
                                          {1000, 2000, 4, 1, 4, 1002, 3, 4, 1102},    {1000, 2000, 4, 1, 4, 1002, 3, 4, 1112},
                                          {1000, 2000, 4, 1, 4, 1002, 3, 4, 1102},    {1000, 2000, 4, 1, 4, 1002, 3, 4, 1112},
                                          {1000, 2000, 4, 1, 4, 1002, 3, 4, 1102},    {1000, 2000, 4, 1, 4, 1002, 3, 4, 1112},
                                          {1000, 2000, 12, 1, 12, 1001, 5, 12, 1011}, {1000, 2000, 12, 1, 12, 1001, 5, 12, 1011},
                                          {1000, 2000, 12, 1, 12, 1001, 5, 12, 1011}, {1000, 2000, 12, 1, 12, 1001, 5, 12, 1011},
                                          {2000, 3000, 1, 2, 1, 2000, 2, 1, 2010},    {2000, 3000, 1, 2, 1, 2000, 2, 1, 2010},
                                          {2000, 3000, 1, 2, 1, 2000, 2, 1, 2010},    {2000, 3000, 1, 2, 1, 2000, 2, 1, 2010},
                                          {2000, 3000, 11, 2, 11, 2001, 2, 11, 2301}, {2000, 3000, 11, 2, 11, 2001, 2, 11, 2301},
                                          {2000, 3000, 11, 2, 11, 2001, 2, 11, 2301}, {2000, 3000, 11, 2, 11, 2001, 2, 11, 2301}};

    //    std::vector<Output> actualOutput = testHarness.getOutput<Output>(expectedOutput.size(), "TopDown", "NONE", "IN_MEMORY");
    return 0;
}
