{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33df9674-1d14-407c-9909-3e7661fde1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd985b7-cca6-4889-9eb4-dbab61ec1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All global variables that would be passed as arguments in a python script\n",
    "input_folder_name = \"/home/nils/Downloads/WindowManagementBM_1737308559\"\n",
    "output_folder_name = \"/home/nils/Downloads/WindowManagementBM_1737308559\"\n",
    "statistics_csv_name = \"all_statistics.csv\"\n",
    "statistics_csv_path = os.path.join(input_folder_name, statistics_csv_name)\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58336ad8-5826-4a66-8076-09d08ad113f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nils/Downloads/WindowManagementBM_1737308559/worker_0.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737308559/worker_1.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737308559/worker_3.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737308559/worker_2.txt\n"
     ]
    }
   ],
   "source": [
    "# Converting \n",
    "pattern_worker_file = r\"^worker_\\d+\\.txt$\"\n",
    "pattern_task_details = (r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+).*?\"\n",
    "       r\"Task (?P<task_id>\\d+) for Pipeline (?P<pipeline>\\d+).*?\"\n",
    "       r\"(?P<action>Started|Completed)(?:\\. Number of Tuples: (?P<num_tuples>\\d+))?\")\n",
    "statistic_files = [os.path.join(input_folder_name, f) for f in os.listdir(input_folder_name) if re.match(pattern_worker_file, f)]\n",
    "combined_df = pd.DataFrame()\n",
    "for stat_file in statistic_files:\n",
    "    print(stat_file)\n",
    "    with open(stat_file, 'r') as file:\n",
    "        log_text = file.read()\n",
    "\n",
    "    records = []\n",
    "    tasks = {}\n",
    "    for match in re.finditer(pattern_task_details, log_text):\n",
    "        timestamp = pd.to_datetime(match.group(\"timestamp\"), format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        task_id = int(match.group(\"task_id\"))\n",
    "        action = match.group(\"action\")\n",
    "        num_tuples = int(match.group(\"num_tuples\")) if match.group(\"num_tuples\") else None\n",
    "        pipeline_id = int(match.group(\"pipeline\")) if match.group(\"pipeline\") else None\n",
    "    \n",
    "        if action == \"Started\":\n",
    "            tasks[task_id] = {\"start_time\": timestamp, \"num_tuples\": num_tuples}\n",
    "        elif action == \"Completed\" and task_id in tasks:\n",
    "            task_info = tasks[task_id]\n",
    "            start_time = task_info[\"start_time\"]\n",
    "            duration = (timestamp - start_time).total_seconds()\n",
    "            throughput = task_info[\"num_tuples\"] / duration if duration > 0 else 0\n",
    "            records.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": timestamp,\n",
    "                \"duration\": duration,\n",
    "                \"num_tuples\": task_info[\"num_tuples\"],\n",
    "                \"throughput\": throughput,\n",
    "                \"pipeline_id\": pipeline_id\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and write it to the csv file\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(os.path.join(input_folder_name, stat_file + \".csv\"), index=False)\n",
    "\n",
    "    # Adding this dataframe to the global one\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Writing the combined dataframe to a csv file\n",
    "combined_df.to_csv(statistics_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ab95a1-7788-41da-9427-109133e01d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all data and plotting the duration and the throughput\n",
    "data = pd.read_csv(statistics_csv_path)\n",
    "# Create a unique plot for each pipeline_id\n",
    "for pipeline in data['pipeline_id'].unique():\n",
    "    # Filter data for the current pipeline_id\n",
    "    pipeline_df = data[data['pipeline_id'] == pipeline]\n",
    "    \n",
    "    # Create subplots for duration and throughput\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot Duration over Task ID (using scatterplot)\n",
    "    sns.scatterplot(x='task_id', y='duration', data=pipeline_df, ax=axes[0])\n",
    "    axes[0].set_title(f\"Duration vs Task ID for Pipeline {pipeline}\")\n",
    "    axes[0].set_xlabel(\"Task ID\")\n",
    "    axes[0].set_ylabel(\"Duration\")\n",
    "    \n",
    "    # Plot Throughput over Task ID (using scatterplot)\n",
    "    sns.scatterplot(x='task_id', y='throughput', data=pipeline_df, ax=axes[1])\n",
    "    axes[1].set_title(f\"Throughput vs Task ID for Pipeline {pipeline}\")\n",
    "    axes[1].set_xlabel(\"Task ID\")\n",
    "    axes[1].set_ylabel(\"Throughput\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder_name, f\"pipeline_{pipeline}_plot.pdf\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1521e53-7a8a-4652-b320-c312d9e56703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_105101/91503787.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n"
     ]
    }
   ],
   "source": [
    "# Loading all data and plotting the duration and the throughput via aggregates\n",
    "# Define the bin size (e.g., every 10 task_ids)\n",
    "bin_size = 100\n",
    "\n",
    "# Create a unique plot for each pipeline_id\n",
    "for pipeline in data['pipeline_id'].unique():\n",
    "    # Filter data for the current pipeline_id\n",
    "    pipeline_df = data[data['pipeline_id'] == pipeline]\n",
    "    \n",
    "    # Create task_id bins\n",
    "    pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
    "    \n",
    "    # Calculate aggregate statistics for each task_id bin\n",
    "    aggregated_data = pipeline_df.groupby('task_id_bin').agg(\n",
    "        avg_duration=('duration', 'mean'),\n",
    "        median_duration=('duration', 'median'),\n",
    "        p90_duration=('duration', lambda x: np.percentile(x, 90)),\n",
    "        avg_throughput=('throughput', 'mean'),\n",
    "        median_throughput=('throughput', 'median'),\n",
    "        p90_throughput=('throughput', lambda x: np.percentile(x, 90))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create subplots for average, median, and 90th percentile duration and throughput\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot Average, Median, and 90th Percentile Duration\n",
    "    sns.lineplot(x='task_id_bin', y='avg_duration', data=aggregated_data, ax=axes[0], label='Average Duration', color='blue')\n",
    "    sns.lineplot(x='task_id_bin', y='median_duration', data=aggregated_data, ax=axes[0], label='Median Duration', color='red')\n",
    "    sns.lineplot(x='task_id_bin', y='p90_duration', data=aggregated_data, ax=axes[0], label='90th Percentile Duration', color='purple')\n",
    "    axes[0].set_title(f\"Duration (Avg, Median, & 90th Percentile) vs Task ID Bin for Pipeline {pipeline}\")\n",
    "    axes[0].set_xlabel(\"Task ID Bin\")\n",
    "    axes[0].set_ylabel(\"Duration\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot Average, Median, and 90th Percentile Throughput\n",
    "    sns.lineplot(x='task_id_bin', y='avg_throughput', data=aggregated_data, ax=axes[1], label='Average Throughput', color='green')\n",
    "    sns.lineplot(x='task_id_bin', y='median_throughput', data=aggregated_data, ax=axes[1], label='Median Throughput', color='orange')\n",
    "    sns.lineplot(x='task_id_bin', y='p90_throughput', data=aggregated_data, ax=axes[1], label='90th Percentile Throughput', color='brown')\n",
    "    axes[1].set_title(f\"Throughput (Avg, Median, & 90th Percentile) vs Task ID Bin for Pipeline {pipeline}\")\n",
    "    axes[1].set_xlabel(\"Task ID Bin\")\n",
    "    axes[1].set_ylabel(\"Throughput\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder_name, f\"pipeline_{pipeline}_aggregated_bin_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6ece4-52ad-4268-9272-c2215ffbbe02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
