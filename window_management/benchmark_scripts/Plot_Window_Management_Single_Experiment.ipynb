{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33df9674-1d14-407c-9909-3e7661fde1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd985b7-cca6-4889-9eb4-dbab61ec1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All global variables that would be passed as arguments in a python script\n",
    "input_folder_name = \"/home/nils/Downloads/WindowManagementBM_1737492626\"\n",
    "output_folder_name = \"/home/nils/Downloads/WindowManagementBM_1737492626\"\n",
    "statistics_csv_name = \"all_statistics.csv\"\n",
    "statistics_csv_path = os.path.join(input_folder_name, statistics_csv_name)\n",
    "pipeline_txt_name = \"pipelines.txt\"\n",
    "pipeline_txt_path = os.path.join(input_folder_name, pipeline_txt_name)\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b327e09c-dac7-4d55-9176-3bebc7cbb6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 'JoinProbe_Emit',\n",
       " 3: 'WindowTrigger',\n",
       " 4: 'Scan_JoinBuild',\n",
       " 5: 'Scan_WatermarkAssignment_Emit',\n",
       " 7: 'WindowTrigger',\n",
       " 8: 'Scan_JoinBuild',\n",
       " 9: 'Scan_WatermarkAssignment_Emit'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting pipelines.txt to a dict of pipeline id to a title\n",
    "def extract_pipeline_data(input_text):\n",
    "    pipeline_dict = {}\n",
    "    # Split the input text by the delimiter\n",
    "    pipeline_sections = input_text.split(\"############################################\")\n",
    "    physical_pattern = re.compile(r\"\\bPhysical\\w+\")\n",
    "\n",
    "    for section in pipeline_sections:\n",
    "        # Find the pipeline ID in the section\n",
    "        pipeline_match = re.search(r\"Pipeline:\\s*(\\d+)\", section)\n",
    "        if pipeline_match:\n",
    "            pipeline_id = int(pipeline_match.group(1))\n",
    "            if pipeline_id not in pipeline_dict:\n",
    "                pipeline_dict[pipeline_id] = []\n",
    "            \n",
    "            # Find all words starting with 'Physical' in the section\n",
    "            physical_matches = physical_pattern.findall(section)\n",
    "            cleaned_matches = [match.replace(\"Physical\", \"\").replace(\"Operator\", \"\").replace(\"Stream\", \"\") for match in physical_matches]\n",
    "            pipeline_dict[pipeline_id].extend(cleaned_matches)\n",
    "\n",
    "    \n",
    "    # Concatenate multiple values with \"_\"\n",
    "    return {key: \"_\".join(values) for key, values in pipeline_dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "with open(pipeline_txt_path, 'r') as input_file:\n",
    "    pipeline_ids_title = extract_pipeline_data(input_file.read())\n",
    "\n",
    "pipeline_ids_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58336ad8-5826-4a66-8076-09d08ad113f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nils/Downloads/WindowManagementBM_1737492626/worker_0.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737492626/worker_1.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737492626/worker_3.txt\n",
      "/home/nils/Downloads/WindowManagementBM_1737492626/worker_2.txt\n"
     ]
    }
   ],
   "source": [
    "# Converting query engine statistics to statistics csv\n",
    "pattern_worker_file = r\"^worker_\\d+\\.txt$\"\n",
    "pattern_task_details = (r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+).*?\"\n",
    "       r\"Task (?P<task_id>\\d+) for Pipeline (?P<pipeline>\\d+).*?\"\n",
    "       r\"(?P<action>Started|Completed)(?:\\. Number of Tuples: (?P<num_tuples>\\d+))?\")\n",
    "statistic_files = [os.path.join(input_folder_name, f) for f in os.listdir(input_folder_name) if re.match(pattern_worker_file, f)]\n",
    "combined_df = pd.DataFrame()\n",
    "for stat_file in statistic_files:\n",
    "    print(stat_file)\n",
    "    with open(stat_file, 'r') as file:\n",
    "        log_text = file.read()\n",
    "\n",
    "    records = []\n",
    "    tasks = {}\n",
    "    for match in re.finditer(pattern_task_details, log_text):\n",
    "        timestamp = pd.to_datetime(match.group(\"timestamp\"), format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        task_id = int(match.group(\"task_id\"))\n",
    "        action = match.group(\"action\")\n",
    "        num_tuples = int(match.group(\"num_tuples\")) if match.group(\"num_tuples\") else None\n",
    "        pipeline_id = int(match.group(\"pipeline\")) if match.group(\"pipeline\") else None\n",
    "    \n",
    "        if action == \"Started\":\n",
    "            tasks[task_id] = {\"start_time\": timestamp, \"num_tuples\": num_tuples}\n",
    "        elif action == \"Completed\" and task_id in tasks:\n",
    "            task_info = tasks[task_id]\n",
    "            start_time = task_info[\"start_time\"]\n",
    "            duration = (timestamp - start_time).total_seconds()\n",
    "            throughput = task_info[\"num_tuples\"] / duration if duration > 0 else 0\n",
    "            records.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": timestamp,\n",
    "                \"duration\": duration * (1000 * 1000),\n",
    "                \"num_tuples\": task_info[\"num_tuples\"],\n",
    "                \"throughput\": throughput / (1000 * 1000),\n",
    "                \"pipeline_id\": pipeline_id\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and write it to the csv file\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(os.path.join(input_folder_name, stat_file + \".csv\"), index=False)\n",
    "\n",
    "    # Adding this dataframe to the global one\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Writing the combined dataframe to a csv file\n",
    "combined_df.to_csv(statistics_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ab95a1-7788-41da-9427-109133e01d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all data and plotting the duration and the throughput\n",
    "data = pd.read_csv(statistics_csv_path)\n",
    "# Create a unique plot for each pipeline_id\n",
    "for pipeline in data['pipeline_id'].unique():\n",
    "    # Filter data for the current pipeline_id\n",
    "    pipeline_df = data[data['pipeline_id'] == pipeline]\n",
    "    \n",
    "    # Create subplots for duration and throughput\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Create title if it is an operator pipeline\n",
    "    title = f\"{pipeline_ids_title[pipeline]}({pipeline})\" if pipeline in pipeline_ids_title else f\"Pipeline ({pipeline})\"\n",
    "    \n",
    "    # Plot Duration over Task ID (using scatterplot)\n",
    "    sns.scatterplot(x='task_id', y='duration', data=pipeline_df, ax=axes[0])\n",
    "    axes[0].set_title(f\"Duration vs Task ID for {title}\")\n",
    "    axes[0].set_xlabel(\"Task ID\")\n",
    "    axes[0].set_ylabel(\"Duration [us]\")\n",
    "    \n",
    "    # Plot Throughput over Task ID (using scatterplot)\n",
    "    sns.scatterplot(x='task_id', y='throughput', data=pipeline_df, ax=axes[1])\n",
    "    axes[1].set_title(f\"Throughput vs Task ID for {title}\")\n",
    "    axes[1].set_xlabel(\"Task ID\")\n",
    "    axes[1].set_ylabel(\"Throughput [Mtup/s]\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder_name, f\"pipeline_{pipeline}_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1521e53-7a8a-4652-b320-c312d9e56703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
      "/tmp/ipykernel_223852/2903438368.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n"
     ]
    }
   ],
   "source": [
    "# Loading all data and plotting the duration and the throughput via aggregates\n",
    "# Define the bin size (e.g., every 10 task_ids)\n",
    "bin_size = 100\n",
    "\n",
    "# Create a unique plot for each pipeline_id\n",
    "for pipeline in data['pipeline_id'].unique():\n",
    "    # Filter data for the current pipeline_id\n",
    "    pipeline_df = data[data['pipeline_id'] == pipeline]\n",
    "    \n",
    "    # Create task_id bins\n",
    "    pipeline_df['task_id_bin'] = (pipeline_df['task_id'] // bin_size) * bin_size\n",
    "    \n",
    "    # Calculate aggregate statistics for each task_id bin\n",
    "    aggregated_data = pipeline_df.groupby('task_id_bin').agg(\n",
    "        avg_duration=('duration', 'mean'),\n",
    "        median_duration=('duration', 'median'),\n",
    "        p90_duration=('duration', lambda x: np.percentile(x, 90)),\n",
    "        avg_throughput=('throughput', 'mean'),\n",
    "        median_throughput=('throughput', 'median'),\n",
    "        p90_throughput=('throughput', lambda x: np.percentile(x, 90))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create subplots for average, median, and 90th percentile duration and throughput\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Create title if it is an operator pipeline\n",
    "    title = f\"{pipeline_ids_title[pipeline]}({pipeline})\" if pipeline in pipeline_ids_title else f\"Pipeline ({pipeline})\"\n",
    "\n",
    "    \n",
    "    # Plot Average, Median, and 90th Percentile Duration\n",
    "    sns.lineplot(x='task_id_bin', y='avg_duration', data=aggregated_data, ax=axes[0], label='Average Duration', color='blue')\n",
    "    sns.lineplot(x='task_id_bin', y='median_duration', data=aggregated_data, ax=axes[0], label='Median Duration', color='red')\n",
    "    sns.lineplot(x='task_id_bin', y='p90_duration', data=aggregated_data, ax=axes[0], label='90th Percentile Duration', color='purple')\n",
    "    axes[0].set_title(f\"Duration vs Task ID Binned\\n for {title}\")\n",
    "    axes[0].set_xlabel(\"Task ID Bin\")\n",
    "    axes[0].set_ylabel(\"Duration [us]\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot Average, Median, and 90th Percentile Throughput\n",
    "    sns.lineplot(x='task_id_bin', y='avg_throughput', data=aggregated_data, ax=axes[1], label='Average Throughput', color='green')\n",
    "    sns.lineplot(x='task_id_bin', y='median_throughput', data=aggregated_data, ax=axes[1], label='Median Throughput', color='orange')\n",
    "    sns.lineplot(x='task_id_bin', y='p90_throughput', data=aggregated_data, ax=axes[1], label='90th Percentile Throughput', color='brown')\n",
    "    axes[1].set_title(f\"Throughput vs Task ID Binned\\n for {title}\")\n",
    "    axes[1].set_xlabel(\"Task ID Bin\")\n",
    "    axes[1].set_ylabel(\"Throughput [Mtup/s]\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder_name, f\"pipeline_{pipeline}_aggregated_bin_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516c1f9a-b8d4-4c28-b297-700222f54012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all data and plotting the duration and the throughput via boxplots\n",
    "# Create a figure with 2 subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Boxplot for Duration\n",
    "sns.boxplot(x='pipeline_id', y='duration', data=data, ax=axes[0])\n",
    "axes[0].set_title(\"Duration Distribution per Pipeline\")\n",
    "axes[0].set_xlabel(\"Pipeline ID\")\n",
    "axes[0].set_ylabel(\"Duration\")\n",
    "\n",
    "# Boxplot for Throughput\n",
    "sns.boxplot(x='pipeline_id', y='throughput', data=data, ax=axes[1])\n",
    "axes[1].set_title(\"Throughput Distribution per Pipeline\")\n",
    "axes[1].set_xlabel(\"Pipeline ID\")\n",
    "axes[1].set_ylabel(\"Throughput\")\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder_name, f\"pipelines_aggregated_boxplot.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6ece4-52ad-4268-9272-c2215ffbbe02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b06b2c-5340-43bc-998c-50049973f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
