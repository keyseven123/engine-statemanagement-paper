{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce8df3f-c01c-4662-a5ba-fafab6fb5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import shutil\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9bbad4-aa8b-4fa4-9daf-3e1cffbd93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All global variables that would be passed as arguments in a python script\n",
    "# Do not think that I need them, after I have moved the load_cache and load_worker to the PostProcessing\n",
    "#pipeline_txt_name = \"pipelines.txt\"\n",
    "#cache_hits_misses_name = \"cache_hits_and_misses.txt\"\n",
    "\n",
    "output_folder = \"/home/nils/Downloads/output_plots\"\n",
    "input_folder = \"/home/nils/Downloads/\"\n",
    "time_stamp = \"1740083964\"\n",
    "worker_statistics_csv_name = f\"worker_statistics_{time_stamp}.csv\"\n",
    "smaller_worker_statistics_csv_name = f\"worker_statistics_{time_stamp}_small.csv\"\n",
    "cache_statistics_csv_name = f\"cache_statistics_{time_stamp}.csv\"\n",
    "worker_statistics_csv_path = os.path.join(input_folder, worker_statistics_csv_name)\n",
    "cache_statistics_csv_path = os.path.join(input_folder, cache_statistics_csv_name)\n",
    "\n",
    "\n",
    "# Remove the output_folder and then create it again\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82a21c5-a3e5-46bf-861b-a0ac0273fdbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Converts a large number to a string \n",
    "def format_si_units(number):\n",
    "    # Define SI unit prefixes\n",
    "    si_prefixes = ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']\n",
    "\n",
    "    # Handle zero as a special case\n",
    "    if number == 0:\n",
    "        return \"0\"\n",
    "\n",
    "    # Determine the sign of the number\n",
    "    sign = ''\n",
    "    if number < 0:\n",
    "        sign = '-'\n",
    "        number = -number\n",
    "\n",
    "    # Determine the order of magnitude\n",
    "    order_of_magnitude = 0\n",
    "    while number >= 1000 and order_of_magnitude < len(si_prefixes) - 1:\n",
    "        number /= 1000.0\n",
    "        order_of_magnitude += 1\n",
    "\n",
    "    # Format the number to one decimal place and append the SI prefix\n",
    "    formatted_number = f\"{number:.3f}{si_prefixes[order_of_magnitude]}\"\n",
    "\n",
    "    # Add the sign back to the formatted number\n",
    "    return sign + formatted_number\n",
    "\n",
    "\n",
    "# Converting pipelines.txt to a dict of pipeline id to a title\n",
    "def extract_pipeline_data(input_path):\n",
    "    with open(input_path, 'r') as input_file:\n",
    "        input_text = input_file.read()\n",
    "\n",
    "    pipeline_dict = {}\n",
    "    # Split the input text by the delimiter\n",
    "    pipeline_sections = input_text.split(\"############################################\")\n",
    "    physical_pattern = re.compile(r\"\\bPhysical\\w+\")\n",
    "\n",
    "    for section in pipeline_sections:\n",
    "        # Find the pipeline ID in the section\n",
    "        pipeline_match = re.search(r\"Pipeline:\\s*(\\d+)\", section)\n",
    "        if pipeline_match:\n",
    "            pipeline_id = int(pipeline_match.group(1))\n",
    "            if pipeline_id not in pipeline_dict:\n",
    "                pipeline_dict[pipeline_id] = []\n",
    "            \n",
    "            # Find all words starting with 'Physical' in the section\n",
    "            physical_matches = physical_pattern.findall(section)\n",
    "            cleaned_matches = [match.replace(\"Physical\", \"\").replace(\"Operator\", \"\").replace(\"Stream\", \"\") for match in physical_matches]\n",
    "            pipeline_dict[pipeline_id].extend(cleaned_matches)\n",
    "    \n",
    "    # Concatenate multiple values with \"_\"\n",
    "    return {key: \"_\".join(values) for key, values in pipeline_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c90488-a74e-4af9-9ca5-923e09194ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all cache statistic files across all folders\n",
    "def load_cache_statistics():\n",
    "    return pd.read_csv(cache_statistics_csv_path)\n",
    "\n",
    "# Converting query engine statistics to statistics csv\n",
    "def load_worker_statistics():\n",
    "    needed_columns = [\"query\", \"slice_cache\", \"number_of_worker_threads\", \"degree_of_disorder\", \"throughput\", \"duration\"]\n",
    "    df = pd.read_csv(worker_statistics_csv_path, usecols=needed_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba547f1e-1b00-4080-b0a4-c86a04643b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the combined data into dataframe named df and printing a little summary of the data frame\n",
    "df = load_worker_statistics()\n",
    "    \n",
    "# General info\n",
    "print(\"### DataFrame Summary ###\\n\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "print(\"Columns and Data Types:\")\n",
    "print(df.dtypes, \"\\n\")\n",
    "\n",
    "# Number of unique values per column\n",
    "print(\"Number of Unique Values per Column:\")\n",
    "print(df.nunique(), \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Descriptive statistics for numeric columns\n",
    "print(\"Descriptive Statistics (Numeric Columns):\")\n",
    "print(df.describe().T, \"\\n\")\n",
    "\n",
    "# Display a sample of rows\n",
    "print(\"Sample Rows:\")\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cfbd0-dea8-4e2e-84f5-dd1206ff24ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# Plotting the total no. tuples, no. tasks, total duration per no. worker threads\n",
    "df = load_worker_statistics()\n",
    "# Aggregate the data by number_of_worker_threads\n",
    "aggregated_data = df.groupby([\"number_of_worker_threads\", \"buffer_size_in_bytes\"]).agg(\n",
    "    total_tuples=('num_tuples', 'sum'),\n",
    "    total_tasks=('task_id', 'count'),\n",
    "    total_duration=('duration', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# Plot total number of tuples\n",
    "sns.barplot(\n",
    "    data=aggregated_data,\n",
    "    x=\"number_of_worker_threads\",\n",
    "    y=\"total_tuples\",\n",
    "    palette=\"pastel\",\n",
    "    hue=\"buffer_size_in_bytes\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Total Number of Tuples per Worker Threads\")\n",
    "axes[0].set_xlabel(\"Number of Worker Threads\")\n",
    "axes[0].set_ylabel(\"Total Tuples\")\n",
    "\n",
    "# Plot total number of tasks\n",
    "sns.barplot(\n",
    "    data=aggregated_data,\n",
    "    x=\"number_of_worker_threads\",\n",
    "    y=\"total_tasks\",\n",
    "    palette=\"pastel\",\n",
    "    hue=\"buffer_size_in_bytes\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Total Number of Tasks per Worker Threads\")\n",
    "axes[1].set_xlabel(\"Number of Worker Threads\")\n",
    "axes[1].set_ylabel(\"Total Tasks\")\n",
    "\n",
    "# Plot total duration\n",
    "sns.barplot(\n",
    "    data=aggregated_data,\n",
    "    x=\"number_of_worker_threads\",\n",
    "    y=\"total_duration\",\n",
    "    palette=\"pastel\",\n",
    "    hue=\"buffer_size_in_bytes\",\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Total Duration per Worker Threads\")\n",
    "axes[2].set_xlabel(\"Number of Worker Threads\")\n",
    "axes[2].set_ylabel(\"Total Duration (ms)\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb42147-8b96-4d88-a2c3-73b77bd518dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "# Plotting the throughput and duration for each pipeline in intervals\n",
    "# Only do this for buffer_size_in_bytes == 8196\n",
    "df = load_worker_statistics()\n",
    "df = df[df['buffer_size_in_bytes'] == 8196]\n",
    "\n",
    "# Normalize the timestamps to timedelta and milliseconds\n",
    "df['start_time_normalized'] = pd.to_timedelta(df['start_time_normalized'])\n",
    "df['start_time_normalized_ms'] = df['start_time_normalized'].dt.total_seconds() * 1000\n",
    "df['end_time_normalized'] = pd.to_timedelta(df['end_time_normalized'])\n",
    "\n",
    "# Create intervals\n",
    "interval_size_in_ms = 500\n",
    "intervals = pd.cut(df['start_time_normalized_ms'], bins=range(0, int(df['start_time_normalized_ms'].max()) + interval_size_in_ms, interval_size_in_ms), right=False)\n",
    "df['interval'] = intervals\n",
    "df['interval'] = df['interval'].apply(lambda x: x.left)\n",
    "\n",
    "# Group by pipeline_id and create a plot for each group\n",
    "interesting_pipeline_ids = df[\"pipeline_id\"].unique()\n",
    "df = df[df['pipeline_id'].isin(interesting_pipeline_ids)]\n",
    "for pipeline_id, group in df.groupby(\"pipeline_id\"):\n",
    "    # Select only the necessary columns for computation\n",
    "    group = group[['interval', 'number_of_worker_threads', 'throughput', 'duration', 'num_tuples']]\n",
    "    \n",
    "    sum_group = group.groupby(['interval', 'number_of_worker_threads'], as_index=True).agg({\n",
    "        'num_tuples': 'sum',\n",
    "        'duration': 'sum',\n",
    "    })\n",
    "    # Calculate throughput as sum(num_tuples) / sum(duration)\n",
    "    sum_group['throughput'] = sum_group['num_tuples'] / sum_group['duration']\n",
    "\n",
    "    # Calculate the average duration\n",
    "    avg_group = group.groupby(['interval', 'number_of_worker_threads'], as_index=False)['duration'].mean()\n",
    "\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Plot throughput vs interval\n",
    "    sns.boxplot(\n",
    "        data=sum_group,\n",
    "        x=\"interval\",\n",
    "        y=\"throughput\",\n",
    "    palette=\"pastel\",\n",
    "        hue=\"number_of_worker_threads\",\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f\"Pipeline ID {pipeline_id}: Throughput vs Interval ({interval_size_in_ms}ms)\")\n",
    "    axes[0].set_xlabel(\"Interval\")\n",
    "    axes[0].set_ylabel(\"Throughput [tup/s]\")\n",
    "\n",
    "    # Plot duration vs interval\n",
    "    sns.boxplot(\n",
    "        data=avg_group,\n",
    "        x=\"interval\",\n",
    "        y=\"duration\",\n",
    "    palette=\"pastel\",\n",
    "        hue=\"number_of_worker_threads\",\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f\"Pipeline ID {pipeline_id}: Duration vs Interval ({interval_size_in_ms}ms)\")\n",
    "    axes[1].set_xlabel(\"Interval\")\n",
    "    axes[1].set_ylabel(\"Duration (ms)\")\n",
    "\n",
    "    # Plot no. tuples vs interval\n",
    "    sns.boxplot(\n",
    "        data=sum_group,\n",
    "        x=\"interval\",\n",
    "        y=\"num_tuples\",\n",
    "    palette=\"pastel\",\n",
    "        hue=\"number_of_worker_threads\",\n",
    "        ax=axes[2]\n",
    "    )\n",
    "    axes[2].set_title(f\"Pipeline ID {pipeline_id}: No. Tuples vs Interval ({interval_size_in_ms}ms)\")\n",
    "    axes[2].set_xlabel(\"Interval\")\n",
    "    axes[2].set_ylabel(\"Num Tuples\")\n",
    "\n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    figure_path = os.path.join(output_folder, f\"pipeline_{pipeline_id}_intervals_{interval_size_in_ms}ms.png\")\n",
    "    plt.savefig(figure_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec44e4-a629-4eab-93a8-871b289cfe33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "#Plotting duration and throughput as a lineplot over the start_time_normalized\n",
    "# Unique pipeline IDs\n",
    "df = load_worker_statistics()\n",
    "unique_pipeline_ids = df['pipeline_id'].unique()\n",
    "\n",
    "# Generate one plot per pipeline\n",
    "for pipeline_id in unique_pipeline_ids:\n",
    "    # Filter data for the current pipeline_id\n",
    "    pipeline_data = df[df['pipeline_id'] == pipeline_id]\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot throughput\n",
    "    sns.lineplot(\n",
    "        data=pipeline_data,\n",
    "        x='start_time_normalized',\n",
    "        y='throughput',\n",
    "    palette=\"pastel\",\n",
    "        hue='buffer_size_in_bytes',\n",
    "        marker=\"o\",\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f\"Pipeline ID {pipeline_id}: Throughput vs Time\")\n",
    "    axes[0].set_xlabel(\"Start Time\")\n",
    "    axes[0].set_ylabel(\"Throughput\")\n",
    "    \n",
    "    # Plot duration\n",
    "    sns.lineplot(\n",
    "        data=pipeline_data,\n",
    "        x='start_time_normalized',\n",
    "        y='duration',\n",
    "    palette=\"pastel\",\n",
    "        hue='buffer_size_in_bytes',\n",
    "        marker=\"o\",\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f\"Pipeline ID {pipeline_id}: Duration vs Time\")\n",
    "    axes[1].set_xlabel(\"Start Time\")\n",
    "    axes[1].set_ylabel(\"Duration\")\n",
    "    \n",
    "    # Adjust layout for readability\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    figure_path = os.path.join(output_folder, f\"pipeline_{pipeline_id}.png\")\n",
    "    #plt.savefig(figure_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close the figure to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9f252-a65d-4ee7-819f-1be8972914dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_stats_df = load_worker_statistics()\n",
    "worker_stats_df['query'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34962f1a-a2d5-404f-930d-839c84e30883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the latency per task over different cache types and for different number of threads\n",
    "worker_stats_df = load_worker_statistics()\n",
    "cache_stats_combined_df = load_cache_statistics()\n",
    "interesting_param = \"number_of_worker_threads\"\n",
    "x_param=\"slice_cache\"\n",
    "hue = \"degree_of_disorder\"\n",
    "y_params = [\"throughput\", \"duration\"]\n",
    "y_param_units = [\"tup/s\", \"s\"]\n",
    "y_axis_logs = [False, True]\n",
    "\n",
    "\n",
    "worker_stats_df[x_param] = worker_stats_df[x_param].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=5)))\n",
    "\n",
    "\n",
    "    # Create a subplot grid.\n",
    "    unique_params = worker_stats_df[interesting_param].unique()\n",
    "    n_params = len(unique_params)\n",
    "    fig, axes = plt.subplots(len(y_params), n_params, figsize=(16, 12), squeeze=False, sharey='row')\n",
    "    for idx, param in enumerate(unique_params):\n",
    "        subset = worker_stats_df[worker_stats_df[interesting_param] == param]\n",
    "    \n",
    "        for y_idx, (y_param, y_param_unit, y_axis_log) in enumerate(zip(y_params, y_param_units, y_axis_logs)):\n",
    "            ax = axes[y_idx][idx]\n",
    "            sns.boxplot(x=x_param, y=y_param, hue=hue, data=subset, ax=ax, palette=\"pastel\")\n",
    "            ax.set_title(f\"{y_param} for {param} Threads (\")\n",
    "            ax.set_xlabel(x_param)\n",
    "            ax.set_ylabel(f\"{y_param} [{y_param_unit}]\")\n",
    "            #ax.set_ylim(bottom=0)\n",
    "            if y_axis_log:\n",
    "                ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(os.path.join(output_folder_plots, f\"plots.png\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0699bdb-1c0a-4d1a-9b66-e4db0f6122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of hits and misses per cache type\n",
    "cache_stats_combined_df = load_cache_statistics()\n",
    "interesting_param = \"number_of_worker_threads\"\n",
    "x_param=\"slice_cache\"\n",
    "hue = \"degree_of_disorder\"\n",
    "y_params = [\"hit_percentage\", \"hits_total\", \"misses_total\"]\n",
    "cache_stats_combined_df[\"hit_percentage\"] = cache_stats_combined_df[\"hits_total\"] / (cache_stats_combined_df[\"hits_total\"] + \n",
    "                                                                                     cache_stats_combined_df[\"misses_total\"])\n",
    "\n",
    "cache_stats_combined_df[x_param] = cache_stats_combined_df[x_param].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=5)))\n",
    "\n",
    "# Create a subplot grid.\n",
    "unique_params = cache_stats_combined_df[interesting_param].unique()\n",
    "n_params = len(unique_params)\n",
    "fig, axes = plt.subplots(3, n_params, figsize=(16, 12), squeeze=False)\n",
    "for idx, param in enumerate(unique_params):\n",
    "    subset = cache_stats_combined_df[cache_stats_combined_df[interesting_param] == param]\n",
    "\n",
    "    ax = axes[0][idx]\n",
    "    sns.boxplot(x=x_param, y=y_params[0], hue=hue, data=subset, ax=ax, palette=\"pastel\")\n",
    "    ax.set_title(f\"Cache Hit Percentage for {param} Threads\")\n",
    "    ax.set_xlabel(x_param)\n",
    "    ax.set_ylabel(f\"Hits [%]\")\n",
    "   # ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax = axes[1][idx]\n",
    "    sns.boxplot(x=x_param, y=y_params[1], hue=hue, data=subset, ax=ax, palette=\"pastel\",)\n",
    "    ax.set_title(f\"Cache Hits for {param} Threads\")\n",
    "    ax.set_xlabel(x_param)\n",
    "    ax.set_ylabel(f\"#Hits\")\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax = axes[2][idx]\n",
    "    sns.boxplot(x=x_param, y=y_params[2], hue=hue, data=subset, ax=ax, palette=\"pastel\")\n",
    "    ax.set_title(f\"Cache Misses for {param} Threads\")\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel(f\"#Misses\")\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_folder_plots, f\"plots.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722849bb-569b-4f98-be6e-44f38c2f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of hits and misses per cache type\n",
    "cache_stats_combined_df = load_cache_statistics()\n",
    "interesting_param = \"number_of_worker_threads\"\n",
    "x_param=\"slice_cache\"\n",
    "hue = \"numberOfEntriesSliceCache\"\n",
    "y_params = [\"hit_percentage\", \"hits_total\", \"misses_total\"]\n",
    "cache_stats_combined_df[\"hit_percentage\"] = cache_stats_combined_df[\"hits_total\"] / (cache_stats_combined_df[\"hits_total\"] + \n",
    "                                                                                     cache_stats_combined_df[\"misses_total\"])\n",
    "#cache_stats_combined_df[x_param] = cache_stats_combined_df[x_param].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=5)))\n",
    "\n",
    "# Create a subplot grid.\n",
    "unique_params = cache_stats_combined_df[interesting_param].unique()\n",
    "n_params = len(unique_params)\n",
    "fig, axes = plt.subplots(1, n_params, figsize=(16, 12), squeeze=False, sharey='row')\n",
    "for idx, param in enumerate(unique_params):\n",
    "    subset = cache_stats_combined_df[cache_stats_combined_df[interesting_param] == param]\n",
    "    plot_data = subset[[\"hits_total\", \"misses_total\"]]\n",
    "    plot_data.set_index(subset[x_param], inplace=True)\n",
    "    ax = axes[0][idx]\n",
    "    \n",
    "    plot_data.plot(kind='bar', stacked=True, ax=ax, color=['skyblue', 'salmon']) #, logy=True)\n",
    "    ax.set_title(f\"Hits / Misses for {param} Thread(s)\")\n",
    "    #ax.set_ylim(bottom=0.1)\n",
    "    \n",
    "    \n",
    "    # Annotate the bars with hits and misses values\n",
    "    for p in ax.patches:\n",
    "        width, height = p.get_width(), p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        height_str = format_si_units(height)\n",
    "        ax.text(x + width / 2, y + height / 2, f'{height_str}', horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_folder_plots, f\"plots.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fa468-5e62-42a7-ba5f-6f650a8a1604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
